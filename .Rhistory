labs(x = "Molecular Weight", y = "Log Solubility") +  # set axis labels
scale_color_manual(values = c("red", "blue"),
name = "Model",
labels = c("Linear Regression", "Random Forest"))  # add legend
ggplot(df, aes(x = MolWeight, y = solubility)) +
geom_point(pch = 20) +
geom_line(aes(y = rt_preds), color = "red") + # overlay the regression tree predictions (red line)
geom_line(aes(y = rf_preds), color = "blue") + # overlay the random forest predictions (blue line)
labs(x = "Molecular Weight", y = "Log Solubility")
ggplot(df, aes(x = MolWeight, y = solubility)) +
geom_point(pch = 20) +
geom_line(aes(y = rt_preds), color = "red") +  # overlay the regression tree predictions
geom_line(aes(y = rf_preds), color = "blue") +
labs(x = "Molecular Weight", y = "Log Solubility")
ggplot(df, aes(x = MolWeight, y = solubility)) +
geom_point(pch = 20) +
geom_line(aes(y = rt_preds), color = "red") + # overlay the regression tree predictions (red line)
geom_line(aes(y = rf_preds), color = "blue") + # overlay the random forest predictions (blue line)
labs(x = "Molecular Weight", y = "Log Solubility")
ggplot(df, aes(x = MolWeight, y = solubility)) +
geom_point(pch = 20) +
geom_line(aes(y = rt_preds), color = "red", linetype = "dashed",
show.legend = TRUE, # include in legend
name = "Regression Tree Predictions") + # label the red line
geom_line(aes(y = rf_preds), color = "blue", linetype = "solid",
show.legend = TRUE, # include in legend
name = "Random Forest Predictions") + # label the blue line
labs(x = "Molecular Weight", y = "Log Solubility")
ggplot(df, aes(x = MolWeight, y = solubility)) +
geom_point(pch = 20) +
geom_line(aes(y = rt_preds), color = "red") + # overlay the regression tree predictions (red line)
geom_line(aes(y = rf_preds), color = "blue") + # overlay the random forest predictions (blue line)
labs(x = "Molecular Weight", y = "Log Solubility")
# Load required packages
library(randomForest)
library(rpart)
# Load solubility data
data(solubility)
# Fit a regression tree model using molecular weight as predictor
rt_model <- rpart(solTrainY_train ~ MolWeight, data = solTrainXtrans_train)
# Fit a random forest model using molecular weight as predictor
rf_model <- randomForest(solTrainY_train ~ MolWeight, data = solTrainXtrans_train)
# Predict on the test set using both models
rt_preds <- predict(rt_model, newdata = solTestX)
rf_preds <- predict(rf_model, newdata = solTestX)
# Plot the predictor (MolWeight) versus solubility for the test set
library(ggplot2)
df <- data.frame(MolWeight = solTestX$MolWeight,
solubility = solTestY,
rt_preds = rt_preds,
rf_preds = rf_preds)
ggplot(df, aes(x = MolWeight, y = solubility)) +
geom_point(pch = 20) +
geom_line(aes(y = rt_preds), color = "red") + # overlay the regression tree predictions (red line)
geom_line(aes(y = rf_preds), color = "blue") + # overlay the random forest predictions (blue line)
labs(x = "Molecular Weight", y = "Log Solubility")
# Project R Script
library(tidyverse)
library(corrplot)
library(caret)
library(stringr)
library(ggplot2)
# My custom functions
# Function to draw bar-plot given column names as a vector
draw_bar_plot <- function(col=colnames(kaggle.data), data=kaggle.data) {
ggplot(data, aes_string(col)) +
geom_bar(color="Black", fill="lightgreen") +
coord_flip() +
ggtitle(col)
}
# Load Data from Source
# data <- read.csv("C:/Users/18046/Desktop/OR568 Predictive Analytics/Project/potential datasets/malware-detection/data.csv")
kaggle.data <- read.csv("C:/Users/18046/Desktop/OR568 Predictive Analytics/Project/potential datasets/master data/Kaggle-data-FINAL.csv")
View(kaggle.data)
# Load necessary packages
library(caret)
# Load the dataset
data(kaggle.data)
# Split data into training and testing sets
set.seed(123)
trainIndex <- createDataPartition(kaggle.data, p = 0.7, list = FALSE)
train <- kaggle.data[trainIndex, ]
test <- kaggle.data[-trainIndex, ]
# Create a model using Recursive Feature Elimination
ctrl <- rfeControl(functions = caretFuncs, method = "cv", number = 5, verbose = FALSE)
lmProfile <- rfe(train[, -1], train[, 1], sizes = c(1:ncol(train)-1), rfeControl = ctrl)
# Print the results
print(lmProfile)
# Plot the results
plot(lmProfile, type=c("g", "o"))
# Use the top features to build a model
topFeatures <- lmProfile$optVariables
model <- train(target ~ ., data=train[,c("target", topFeatures)], method="glm")
# Test the model on the testing set
predictions <- predict(model, newdata=test[, topFeatures])
# ================================================================================
kaggle.data %>%
keep(is.numeric) %>%
gather() %>%
ggplot(aes(value)) +
geom_histogram(bins = 20, color="black", fill="cornFlowerBlue") +
facet_wrap(~key, scales = 'free') +
ggtitle("Histograms of All Numerical Predictors")
# Remove those pesky Nulls from X column
# identify columns with null values
null_cols <- which(colSums(is.na(kaggle.data)) > 0)
# subset the data frame, dropping the columns with null values
df_subset <- kaggle.data[, -null_cols]
complete_rows <- complete.cases(df_subset)
table(complete_rows)
# Boxplots
kaggle.data %>%
keep(is.numeric) %>%
gather() %>%
ggplot(aes(value)) +
geom_boxplot(outlier.color = "red", color='black') +
facet_wrap(~key, scales = 'free') +
ggtitle("Boxplots of Numerical Predictors")
# Corrplot aggregated
kaggle.data %>%
keep(is.numeric) %>%
cor() %>%
corrplot()
# Proportion of legitimate vs illegitimate websites: numerical & percentages
legitimate.description <- c("Normal", "Malware")
table(kaggle.data$legitimate)
paste(legitimate.description, " ", round(table(kaggle.data$legitimate) / sum(table(kaggle.data$legitimate)) * 100, 2),sep="","%")
draw_bar_plot("legitimate", kaggle.data)
# Identify and plot zero variance predictors
library(caret)
zero.variance.predictors <- colnames(kaggle.data)[nearZeroVar(kaggle.data)]
kaggle.data %>%
select(zero.variance.predictors) %>%
gather() %>%
filter(is.numeric(value)) %>%
ggplot(aes(value)) +
geom_histogram(bins = 20, color="black", fill="cornFlowerBlue") +
facet_wrap(~key, scales = 'free') +
ggtitle("Histograms of Zero Variance Predictors")
# Identify Columns with an option under the observed proportion threshold (5%)
proportion.threshold <- 0.05
degenerate.distributions <- kaggle.data %>%
gather() %>%
group_by(key, value) %>%
summarise(prop = n()/nrow(kaggle.data)) %>%
ungroup() %>%
filter(prop < proportion.threshold)
unique(degenerate.distributions$key)
(degenerate.distributions)
# Identify missing data grouped by machine
incomplete.machines <- as.data.frame(
kaggle.data %>%
group_by(Machine) %>%
mutate(machine_Total = n()) %>%
ungroup() %>%
filter(!complete.cases(.)) %>%
group_by(Machine) %>%
mutate(Missing = n(),
Proportion =  Missing / machine_Total) %>%
ungroup()%>%
select(Machine, Proportion) %>%
distinct() )
incomplete.machines
# Identify missing data grouped by ID
incomplete.ID <- as.data.frame(
kaggle.data %>%
group_by(ID) %>%
mutate(ID_Total = n()) %>%
ungroup() %>%
filter(!complete.cases(.)) %>%
group_by(ID) %>%
mutate(Missing = n(),
Proportion =  Missing / ID_Total) %>%
ungroup()%>%
select(ID, Proportion) %>%
distinct() )
incomplete.ID
# Missing Records Identifier
library(stringr)
kaggle.data %>%
summarise_all(list(~is.na(.)))%>%
pivot_longer(everything(), names_to = "variables", values_to="missing") %>%
count(variables, missing) %>%
mutate(variables = str_wrap(variables, width = 20)) %>%
ggplot(aes(y = variables, x=n, fill = missing))+
geom_col(position = position_dodge(width = 0.5)) +
labs(title = "Proportion of Missing Attributes",
x = "Proportion") +
scale_fill_manual(values=c("forestgreen","red")) +
theme(axis.text.y = element_text(size = 8))
# Which attributes are missing?
attributes.na <- colSums(is.na(kaggle.data))
attributes.na
class_table <- table(kaggle.data$Machine[kaggle.data$Machine %in% incomplete.machines$Machine])
class_table[class_table > 0]
sum(class_table)
total.na <- sum(is.na(kaggle.data))
print(paste("The total number of na values is: ", total.na))
attributes.na <- colSums(is.na(kaggle.data))
attributes.na
percentage.na <- round(100 * attributes.na / total.na, 1)
paste(names(percentage.na), ": ", percentage.na, "%", sep="")
unique(kaggle.data$X)
View(filter(kaggle.data, X == 0)
)
library(ggplot2)
ggplot(kaggle.data, aes(x=SizeOfCode)) +
geom_histogram(binwidth=100000, fill="cornflowerblue", color="white")
#labs(title="Size of Code Distribution", x="Size of Code", y="Count")
# corrplot
num_vars <- kaggle.data[sapply(kaggle.data, is.numeric)]
corr_matrix <- cor(num_vars)
corrplot(corr_matrix)
# ==============================================================================
# ggcorrplot test
library(ggplot2)
library(ggcorrplot)
# Subset numeric variables from kaggle.data
num_vars <- kaggle.data[sapply(kaggle.data, is.numeric)]
# Calculate correlation matrix and replace missing/invalid values with zeros
corr_matrix <- cor(num_vars, use = "pairwise.complete.obs")
corr_matrix[is.na(corr_matrix) | is.infinite(corr_matrix)] <- 0
# Create ggcorrplot with larger space between attribute labels
ggcorrplot(corr_matrix, hc.order = TRUE,
ggtext = ggplot2::element_text(size = 8, hjust = 0.5, margin = margin(t = 0, b = 5)))
ggcorrplot(corr_matrix, hc.order = TRUE)
# ==============================================================================
# ==============================================================================
library(ggplot2)
# Subset numeric variables from kaggle.data
num_vars <- kaggle.data[sapply(kaggle.data, is.numeric)]
# Calculate correlation matrix and replace missing/invalid values with zeros
corr_matrix <- cor(num_vars, use = "pairwise.complete.obs")
corr_matrix[is.na(corr_matrix) | is.infinite(corr_matrix)] <- 0
# Convert correlation matrix to dataframe
df <- reshape2::melt(corr_matrix)
# Create heatmap with coral shade for negative correlations and black box borders
ggplot(df, aes(Var1, Var2, fill = value)) +
geom_tile(color = "black") +
scale_fill_gradient2(low = "coral", mid = "white", high = "steelblue",
midpoint = 0, limits = c(-1,1), guide = "colorbar") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
labs(title = "Malware Data Variable Correlation Heatmap", x = "Variable 1", y = "Variable 2") +
geom_rect(xmin = -Inf, xmax = Inf, ymin = -Inf, ymax = Inf, fill = NA, color = "black")
# ==============================================================================
# Metadata Plots
# Load dataset
# Get number of columns for each data type
num_numeric <- sum(sapply(kaggle.data, is.numeric))
num_factor <- sum(sapply(kaggle.data, is.factor))
num_character <- sum(sapply(kaggle.data, is.character))
num_logical <- sum(sapply(kaggle.data, is.logical))
# Create bar plot
barplot(
c(num_numeric, num_factor, num_character, num_logical),
names.arg = c("Numeric", "Factor", "Character", "Logical"),
xlab = "Data Types",
ylab = "Number of Columns",
main = "Data Types in kaggle.data"
)
library(ggplot2)
# Get number of columns for each data type
num_numeric <- sum(sapply(kaggle.data, is.numeric))
num_factor <- sum(sapply(kaggle.data, is.factor))
num_character <- sum(sapply(kaggle.data, is.character))
num_logical <- sum(sapply(kaggle.data, is.logical))
# Create data frame for bar plot
data <- data.frame(
Data_Types = c("Numeric", "Factor", "Character", "Logical"),
Number_of_Columns = c(num_numeric, num_factor, num_character, num_logical)
)
# Sort the data frame by Number_of_Columns in descending order
data <- data[order(-data$Number_of_Columns), ]
# Create the ggplot bar plot
ggplot(data, aes(x = reorder(Data_Types, Number_of_Columns), y = Number_of_Columns, fill = Data_Types)) +
geom_bar(stat = "identity", color="black") +
geom_text(aes(label = Number_of_Columns), hjust = -0.5, vjust=-0.5, color = "black") +
coord_flip() +
labs(x = "Data Types", y = "Number of Attributes",
title = "Data Types in Malware Classifier Data") +
theme(legend.position = "bottom")
# Basic violin plot
library(ggplot2)
p <- ggplot(kaggle.data, aes(x=SizeOfCode, y=SectionsMeanEntropy)) +
geom_violin()
p
# Rotate the violin plot
p + coord_flip()
# Set trim argument to FALSE
ggplot(kaggle.data, aes(x=x.var, y=y.var)) +
geom_violin(trim=FALSE)
?fviz_contrib
library(lattice)
library(AppliedPredictiveModeling)
library(dplyr)
library(tidyr)
library(ggplot2)
library(corrplot)
library(caret)
library(e1071)
setwd("C:/Dev/OR568MalwareClassifier")
# Create a grouping variable using kmeans
# Create 3 groups of variables (centers = 3)
set.seed(568)
library(factoextra)
library("FactoMineR")
cwd <- getwd()
file.name <- "Kaggle-data-FINAL.csv"
paste(cwd, file.name ,sep="/")
kdatafinal <- read.csv(paste(cwd, file.name ,sep="/"))
# I am removing md5
malwaredf <- Kdatafinal[, -2]
malwaredf
str(malwaredf)
# Find out the length of the data frame
print(length(malwaredf))
# PCA analysis
#find out the mean of cor(malwaredf)
#1. Check PCA eligibility. After running the mean, I got 0.0423 as result which
#is high;therefore it is eligible for PCA
mean(cor(malwaredf)) # mean checks for the avg correlaion between the variables.
#2. Principal Component Analysis
#malwarePCA = princomp(malwaredf)
#It shows that the correlation between variables and the loading creation should equal                to the variables
#malwarePCA$loadings
#Create an object (PCmalware) to check if variables are dependent or not.If they are
#independent the correlation among the components must be zero or must be nearly zero.
#pcmalware = malwarePCA$scores
#View(pcmalware)
#summary(pcmalware)
#cor(pcmalware)
#Running skeweness to all the dataset (malwaredf)
skewValues <- apply(malwaredf,2,skewness)
library(caret)
#Running the PCA for the entire final data set
pcaM <- prcomp(malwaredf, center=TRUE, scale. = TRUE )
summary(pcaM)
#Running the PCA for the whole set minus the id column, so c(2:56).
#I can tell that there is 5 elements and its size is 95.2 MB, 2M smaller than the pcaM.
pcaM1<-prcomp(malwaredf [c(2:56)], center=TRUE, scale. = TRUE)
summary(pcaM1)
#Calculate the cumulative percentage of variance which each component accounts for.The PCA object for
#the Malwaredataset is pcaM1 (does not contain the ID column).
percentVarMalware <- pcaM1$sd^2/sum(pcaM1$sd^2)*100
percentVarMalware[2:56]
plot(percentVarMalware, xlab="Components", ylab="Percentage of Total Variance", type="l", main="Scree Plot of PCA Analysis")
#The another sub-object called rotation stores the variable loadings, where  rows correspond to
#predictor variables and columns are associated with the  components.
head(pcaM) # it brings up $x which has the PCs
head(pcaM1$rotation[, 1:5])
library(corrplot)
#Running correlation for the predictors
corrMalwaredf <- cor(malwaredf)
dim(corrMalwaredf)
#Visualize the correlation structure of the data. A heatmap cor was created by Matt instead of this one.
corrplot(corrMalwaredf, order="hclust")
# filtering hig correlations, cutoff = 0.85
highCorr <- findCorrelation(corrMalwaredf, cutoff = .85)
length(highCorr)
head(highCorr)
filteredMalwaredf <- malwaredf[, -highCorr]#Filtering the highcorr variables, the data set gets
#down to 48 variables.
library(lattice)
library(AppliedPredictiveModeling)
library(dplyr)
library(tidyr)
library(ggplot2)
library(corrplot)
library(caret)
library(e1071)
cwd <- getwd()
file.name <- "Kaggle-data-FINAL.csv"
paste(cwd, file.name ,sep="/")
kdatafinal <- read.csv(paste(cwd, file.name ,sep="/"))
# I am removing md5
malwaredf <- Kdatafinal[, -2]
malwaredf
str(malwaredf)
# Find out the length of the data frame
print(length(malwaredf))
malwaredf
Kdatafinal
kdatafinal
# I am removing md5
malwaredf <- Kdatafinal[, -2]
kdatafinal <- read.csv(paste(cwd, file.name ,sep="/"))
# I am removing md5
malwaredf <- kdatafinal[, -2]
malwaredf
str(malwaredf)
# Find out the length of the data frame
print(length(malwaredf))
# PCA analysis
#find out the mean of cor(malwaredf)
#1. Check PCA eligibility. After running the mean, I got 0.0423 as result which
#is high;therefore it is eligible for PCA
mean(cor(malwaredf)) # mean checks for the avg correlaion between the variables.
#View(pcmalware)
#summary(pcmalware)
#cor(pcmalware)
#Running skeweness to all the dataset (malwaredf)
skewValues <- apply(malwaredf,2,skewness)
library(caret)
#Running the PCA for the entire final data set
pcaM <- prcomp(malwaredf, center=TRUE, scale. = TRUE )
summary(pcaM)
#Running the PCA for the whole set minus the id column, so c(2:56).
#I can tell that there is 5 elements and its size is 95.2 MB, 2M smaller than the pcaM.
pcaM1<-prcomp(malwaredf [c(2:56)], center=TRUE, scale. = TRUE)
summary(pcaM1)
#Calculate the cumulative percentage of variance which each component accounts for.The PCA object for
#the Malwaredataset is pcaM1 (does not contain the ID column).
percentVarMalware <- pcaM1$sd^2/sum(pcaM1$sd^2)*100
percentVarMalware[2:56]
plot(percentVarMalware, xlab="Components", ylab="Percentage of Total Variance", type="l", main="Scree Plot of PCA Analysis")
#The another sub-object called rotation stores the variable loadings, where  rows correspond to
#predictor variables and columns are associated with the  components.
head(pcaM) # it brings up $x which has the PCs
head(pcaM1$rotation[, 1:5])
library(corrplot)
#Running correlation for the predictors
corrMalwaredf <- cor(malwaredf)
dim(corrMalwaredf)
#Visualize the correlation structure of the data. A heatmap cor was created by Matt instead of this one.
corrplot(corrMalwaredf, order="hclust")
# filtering hig correlations, cutoff = 0.85
highCorr <- findCorrelation(corrMalwaredf, cutoff = .85)
length(highCorr)
head(highCorr)
filteredMalwaredf <- malwaredf[, -highCorr]#Filtering the highcorr variables, the data set gets
library(factoextra)
library("FactoMineR")
pcaM <- prcomp(malwaredf, scale = TRUE)
print(pcaM)
#the eigenvalues measure the amount of variation retained by each principal component.
#Eigenvalues are large for the first PCs and small for the subsequent PCs. That is,
#the first PCs corresponds to the directions with the maximum amount of variation in the data set.
# Get Eigenvalues
eig.val <- get_eigenvalue(pcaM)
eig.val
get_eig(pcaM)
# Eigenvalues
eig.val1 <- get_eigenvalue(pcaM1)
eig.val1
#Extract eigenvalues/variances
get_eig(pcaM1)
#Visualize eigenvalues (scree plot).
# The scree plot can be produced using the function fviz_eig() or fviz_screeplot() [factoextra package].
fviz_screeplot(pcaM, addlabels = TRUE, ylim = c(0, 50))
fviz_screeplot(pcaM1, addlabels = TRUE, ylim = c(0, 50))
fviz_eig(pcaM)
fviz_eig(pcaM1)
print(pcaM1)
# Extract the results for variables
varpca <- get_pca_var(pcaM1)
varpca
# Coordinates of variables
head(varpca$coord)
# Graph of variables: default plot
fviz_pca_var(pcaM1, col.var = "purple")
#It's possible to control variable colors using their contributions ("contrib") to the principal axes:
fviz_pca_var(pcaM1, col.var="contrib",
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
repel = TRUE # Avoid text overlapping
)
# Contributions of variables to PC1 for the top 5 variables
fviz_contrib(pcaM1, choice = "var", axes = 1, top = 5)
fviz_contrib(pcaM1, choice = "var", axes = 1:2, top = 5)
# Contributions of variables to PC1
fviz_contrib(pcaM1, choice = "var", axes = 1, top = 5)
?fviz_contrib
# Contributions of variables to PC1
fviz_contrib(pcaM1, choice = "var", axes = 0, top = 5)
# Contributions of variables to PC1
fviz_contrib(pcaM1, choice = "var", axes = 2, top = 5)
# Contributions of variables to PC1
fviz_contrib(pcaM1, choice = "var", axes = 1, top = 5)
# Contributions of variables to PC1
fviz_contrib(pcaM1, choice = "var", axes = 1, top = 5, fill = "red")
# Contributions of variables to PC1
fviz_contrib(pcaM1, choice = "var", axes = 1, top = 5)
# Contributions of variables to PC1
fviz_contrib(pcaM1, choice = "var", axes = 1, top = 5, color = "black")
# Extract the results for individuals
indpca <- get_pca_ind(pcaM1)
indpca
head(indpca$coord,10)
fviz_pca_ind(pcaM1)
#I cannot display it, it keep freezing Rstudio.
#fviz_pca_ind(pcaM1, col.ind = "cos2",
#            gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
# repel = TRUE # Avoid text overlapping (slow if many points)
#)
# Biplot of individuals and variables
#fviz_pca_biplot(pcaM1, repel = TRUE) # It freezes Rstudio
#Visualize the cos2
head(varpca$cos2, 4)
corrplot(varpca$cos2, is.corr=FALSE)
# Color by cos2 values: quality on the factor map. The highest cos2 variable is 0.6.
fviz_pca_var(pcaM1, col.var = "cos2",
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
repel = TRUE # Avoid text overlapping
)
fviz_contrib(pcaM1, choice = "var", axes = 1, top = 10)
# Contributions of variables to PC2
fviz_contrib(pcaM1, choice = "var", axes = 2, top = 10)
#The total contribution to PC1 and PC2 is obtained with the following R code:
fviz_contrib(pcaM1, choice = "var", axes = 1:2, top = 10)
# Create a grouping variable using kmeans
# Create 3 groups of variables (centers = 3)
set.seed(568)
