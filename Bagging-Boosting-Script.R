## Import the package and libraries that will be used for this modeling

library(AppliedPredictiveModeling)
library(caret)
library(e1071)
library(corrplot)
library(lattice)
library(dplyr)
library(caretEnsemble)
library(ROCR)
# Load dataset for your machine 
# Set your working directory to the directory to where all project R Scripts 
cwd <- getwd()
file.name <- "Kaggle-data-FINAL.csv"
paste(cwd, file.name ,sep="/")
df <- read.csv(paste(cwd, file.name ,sep="/"))
# Label the response variable column name
response_var <- "legitimate" 
## Confirm there are no nulls
sum(is.na(df))
## Check for near zero variance using the nearZeroVar function
zeroVar <- nearZeroVar(df, saveMetrics=FALSE)
## Column index 10, 30, 31, 32, 33, and 47 all have zero  variance
colnames(df[c(10,30,31,32,33,47)])
##Dropping near zero variance columns
df_trans1 <- df[, -c(10,30,31,32,33,47)] 
##Drop the ID column because this is an index and is non-informational as well as the md5 column
df_trans1 <- df_trans1[, -c(1,2)]
head(df_trans1)
## Find highly correlated variables; remove corr magnitude .8 or higher
corr_matrix <- cor(df_trans1)
high_corr <- findCorrelation(corr_matrix, cutoff = 0.8)
removed_variables <- names(df_trans1)[high_corr]
removed_variables

##Remove highly correlated variables from data frame
df_trans2 <- df_trans1[, -high_corr]
##Convert outcome variable to 2-level factor
df_trans2$legitimate <- as.factor(df_trans2$legitimate)

##Next the data partition will be created
set.seed(568)
index <- createDataPartition(y=df_trans2$legitimate, p = 0.7, list = FALSE)
train <- df_trans2[index, ]
test <- df_trans2[-index, ]
x_train <- train[, -which(names(train) == response_var)]
y_train <- train[, which(names(train) == response_var)]
x_test <- test[, -which(names(test) == response_var)]
y_test <- test[, which(names(test) == response_var)]
##Bagged Decision Tree
set.seed(568)
ctrl <- trainControl(method = "repeatedcv", number = 2, repeats = 2)
bag_model <- train(legitimate~., data = train, method = "treebag", metric = "Accuracy", trControl = ctrl)
bag_model
##RUNTIME: is about 12-15 minutes on Brett's Machine
## Check the confusion Matrix on the test set
bag_modelCM <- confusionMatrix(data = predict(bag_model, x_test), reference = y_test)
bag_modelCM
#######################################################################################################
# Matt's attempt
# Get predicted probabilities for the test set
bag_probs <- predict(bag_model, newdata = x_test, type = "prob")[,2]
# Create a prediction object
bag_pred <- prediction(bag_probs, y_test)
# Calculate TPR and FPR for different threshold values
bag_perf <- performance(bag_pred, "tpr", "fpr")

library(ggplot2)
# Identify subtitle metrics
bagged.accuracy <- bag_modelCM$overall['Accuracy']
bagged.kappa <- bag_modelCM$overall['Kappa']
bagged.subtitle <- paste("Accuracy: ", round(bagged.accuracy,4),
                         "Kappa: ", round(bagged.kappa,4))
# Get the ROC curve data
roc_data <- data.frame(x = bag_perf@x.values[[1]], y = bag_perf@y.values[[1]])
# Plot the ROC curve
ggplot(roc_data, aes(x = x, y = y)) +
  geom_line(color = "darkblue") +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
  labs(x = "False Positive Rate", y = "True Positive Rate", 
       title = "ROC Curve for Bagged Tree Model",
       subtitle = bagged.subtitle)

#######################################################################################################
##Create a new Threshold to minimize the false positives
#First create a data frame to view thresholds
bag_cutoffs <- data.frame(cut=bag_perf@alpha.values[[1]], fpr=bag_perf@x.values[[1]],
                          tpr=bag_perf@y.values[[1]])
bag_cutoffs
## From this table, we will try a cutoff point of 0.6
bag_thrshld <- ifelse(bag_probs > 0.6, 1, 0)
bag_thrhld_lvl <- factor(bag_thrshld, levels = levels(y_test[0]))
bag_modelCM2 <- confusionMatrix(bag_thrhld_lvl, y_test)

##Next try a threshold of 0.7
bag_thrshld2 <- ifelse(bag_probs > 0.7, 1, 0)
bag_thrhld_lvl2 <- factor(bag_thrshld2, levels = levels(y_test[0]))
bag_modelCM3 <- confusionMatrix(bag_thrhld_lvl2, y_test)
bag_modelCM3

## Next try a threshold of 0.8
bag_thrshld3 <- ifelse(bag_probs > 0.8, 1, 0)
bag_thrhld_lvl3 <- factor(bag_thrshld3, levels = levels(y_test[0]))
bag_modelCM4 <- confusionMatrix(bag_thrhld_lvl3, y_test)
bag_modelCM4

## Next try a threshold of 0.9
bag_thrshld4 <- ifelse(bag_probs > 0.9, 1, 0)
bag_thrhld_lvl4 <- factor(bag_thrshld4, levels = levels(y_test[0]))
bag_modelCM5 <- confusionMatrix(bag_thrhld_lvl4, y_test)
bag_modelCM5

## Next try a threshold of 0.95
bag_thrshld5 <- ifelse(bag_probs > 0.95, 1, 0)
bag_thrhld_lvl5 <- factor(bag_thrshld5, levels = levels(y_test[0]))
bag_modelCM6 <- confusionMatrix(bag_thrhld_lvl5, y_test)
bag_modelCM6
tst_mal <- sum(y_test == 0)
threshold_df <- data.frame(Threshold=c(0.5,0.6,0.7,0.8,0.9,0.95),
                           FPCount=c(bag_modelCM$table[2,1], bag_modelCM2$table[2,1], bag_modelCM3$table[2,1],
                                     bag_modelCM4$table[2,1], bag_modelCM5$table[2,1], bag_modelCM6$table[2,1]),
                           FPRate=c(bag_modelCM$table[2,1]/tst_mal, bag_modelCM2$table[2,1]/tst_mal, bag_modelCM3$table[2,1]/tst_mal,
                                    bag_modelCM4$table[2,1]/tst_mal, bag_modelCM5$table[2,1]/tst_mal, bag_modelCM6$table[2,1]/tst_mal),
                           Kappa=c(bag_modelCM$overall['Kappa'], bag_modelCM2$overall['Kappa'], bag_modelCM3$overall['Kappa'],
                                   bag_modelCM4$overall['Kappa'], bag_modelCM5$overall['Kappa'], bag_modelCM6$overall['Kappa']))
threshold_df

##Create plots
ggplot(threshold_df, aes(x = Threshold, y = FPRate)) +
  geom_line(color = "darkblue") +
  labs(x = "Threshold (%)", y = "False Positive Rate", 
       title = "Graph Showing Resulting False Positive Rates with Varying Thresholds")
#######################################################################################################
            
##Take a look at variable importance
var_importance <- varImp(bag_model)
plot(var_importance, ylab = "Variable", main = "Variable Impotance for Bagged Decision Tree")

#######################################################################################################
##Next a random forest model will be created
set.seed(568)
rand_forest <- train(legitimate~., data = train, method = "rf", metric = "Accuracy",
                     trControl = ctrl)
rand_forest
## Check the confusion Matrix on the test set
rand_forestCM <- confusionMatrix(data = predict(rand_forest, x_test), reference = y_test)
rand_forestCM
## Create plot of variable importance
var_importance2 <-varImp(rand_forest)
plot(var_importance2, ylab = "Variable", main = "Variable Impotance for Random Forest")
#######################################################################################################
#######################################################################################################
############################ Stochastic Gradient Boosting #########################
set.seed(568)
grad_boost_model <- train(legitimate~., data = train, method = "gbm", metric = "Accuracy",
                          trControl = ctrl)
grad_boost_model
## Check the confusion Matrix on the test set
grad_boost_modelCM <- confusionMatrix(data = predict(grad_boost_model, x_test), reference = y_test)
grad_boost_modelCM
summary(grad_boost_model)

grad_probs <- predict(grad_boost_model, newdata = x_test, type = "prob")[,2]
# Create a prediction object
grad_pred <- prediction(grad_probs, y_test)
# Calculate TPR and FPR for different threshold values
grad_perf <- performance(grad_pred, "tpr", "fpr")

# Identify subtitle metrics
grad.accuracy <- grad_boost_modelCM$overall['Accuracy']
grad.kappa <- grad_boost_modelCM$overall['Kappa']
grad.subtitle <- paste("Accuracy: ", round(grad.accuracy,4),
                         "Kappa: ", round(grad.kappa,4))
# Get the ROC curve data
roc_data2 <- data.frame(x = grad_perf@x.values[[1]], y = grad_perf@y.values[[1]])
# Plot the ROC curve
ggplot(roc_data2, aes(x = x, y = y)) +
  geom_line(color = "darkblue") +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
  labs(x = "False Positive Rate", y = "True Positive Rate", 
       title = "ROC Curve for Gradient Boosted Model",
       subtitle = grad.subtitle)
## Try differing thresholds for the model to reduce the False Positive Rate
## We will try a cutoff point of 0.6
grad_thrshld <- ifelse(grad_probs > 0.6, 1, 0)
grad_thrhld_lvl <- factor(grad_thrshld, levels = levels(y_test[0]))
grad_modelCM2 <- confusionMatrix(grad_thrhld_lvl, y_test)
grad_modelCM2
##Next try a threshold of 0.7
grad_thrshld2 <- ifelse(grad_probs > 0.7, 1, 0)
grad_thrhld_lvl2 <- factor(grad_thrshld2, levels = levels(y_test[0]))
grad_modelCM3 <- confusionMatrix(grad_thrhld_lvl2, y_test)
grad_modelCM3

## Next try a threshold of 0.8
grad_thrshld3 <- ifelse(grad_probs > 0.8, 1, 0)
grad_thrhld_lvl3 <- factor(grad_thrshld3, levels = levels(y_test[0]))
grad_modelCM4 <- confusionMatrix(grad_thrhld_lvl3, y_test)
grad_modelCM4

## Next try a threshold of 0.9
grad_thrshld4 <- ifelse(grad_probs > 0.9, 1, 0)
grad_thrhld_lvl4 <- factor(grad_thrshld4, levels = levels(y_test[0]))
grad_modelCM5 <- confusionMatrix(grad_thrhld_lvl4, y_test)
grad_modelCM5

## Next try a threshold of 0.95
grad_thrshld5 <- ifelse(grad_probs > 0.95, 1, 0)
grad_thrhld_lvl5 <- factor(grad_thrshld5, levels = levels(y_test[0]))
grad_modelCM6 <- confusionMatrix(grad_thrhld_lvl5, y_test)
grad_modelCM6
tst_mal <- sum(y_test == 0)
threshold_boost_df <- data.frame(Threshold=c(0.5,0.6,0.7,0.8,0.9,0.95),
                           FPCount=c(grad_boost_modelCM$table[2,1], grad_modelCM2$table[2,1], grad_modelCM3$table[2,1],
                                     grad_modelCM4$table[2,1], grad_modelCM5$table[2,1], grad_modelCM6$table[2,1]),
                           FPRate=c(grad_boost_modelCM$table[2,1]/tst_mal, grad_modelCM2$table[2,1]/tst_mal, grad_modelCM3$table[2,1]/tst_mal,
                                    grad_modelCM4$table[2,1]/tst_mal, grad_modelCM5$table[2,1]/tst_mal, grad_modelCM6$table[2,1]/tst_mal),
                           Kappa=c(grad_boost_modelCM$overall['Kappa'], grad_modelCM2$overall['Kappa'], grad_modelCM3$overall['Kappa'],
                                   grad_modelCM4$overall['Kappa'], grad_modelCM5$overall['Kappa'], grad_modelCM6$overall['Kappa']))
threshold_boost_df
##Create plots
ggplot(threshold_boost_df, aes(x = Threshold, y = FPRate)) +
  geom_line(color = "darkblue") +
  labs(x = "Threshold (%)", y = "False Positive Rate", 
       title = "Graph Showing Resulting False Positive Rates with Varying Thresholds",
       subtitle = "Stochastic Gradient Boosting Model used with 150 Trees")

############################################################################################################
############################################################################################################
##Adding Micaela's model
 
 cwd <- getwd()
 file.name <- "ImportVar10.csv"
 paste(cwd, file.name ,sep="/")
 ImportVar10 <- read.csv(paste(cwd, file.name ,sep="/"))
 
 #===============================================================================
 # Pre-Process Data
 #===============================================================================
 null_cols <- which(colSums(is.na(ImportVar10)) > 0)
 null_cols
 sum(is.na(ImportVar10))
 zeroVar <- nearZeroVar(ImportVar10, saveMetrics=FALSE)
 zeroVar
 corr_matrix <- cor(ImportVar10)
 high_corr <- findCorrelation(corr_matrix, cutoff = 0.8)
 high_corr
 skewValues <- apply(ImportVar10, 2, skewness)
 head(skewValues)
 cor(ImportVar10)
 
 # Label the response variable column name
 response_var <- "legitimate" 
 #Step 1 - load the necessary packages
 #install.packages("randomForest")
 library(randomForest)
 #===============================================================================
 # Split Test Train Data
 #===============================================================================
 set.seed(568)
 ##list = FALSE, a matrix of row numbers is generated.
 #These samples are allocated to the training set.I have to indicate a column with the dataset so 
 
 #-----------------------------------------------------------------------------
 ImportVar10$legitimate <- as.factor(ImportVar10$legitimate)
 str(ImportVar10)
 response_var <- "legitimate" 
 ####----------------------------------------------------------------------
 # Splitting test and train data
 #the createDataPartition works.Creating the predictors df excluding the outcome variable.
 
 library(caret)
 trainingindex <- createDataPartition(y=ImportVar10$legitimate, p = .70,list= FALSE) 
 head(trainingindex)
 traindata <-ImportVar10[trainingindex, ]
 testdata <- ImportVar10[-trainingindex, ]
 x_train <- traindata[, -which(names(traindata) == response_var)]
 y_train <- traindata[, which(names(traindata) == response_var)]
 x_test <- testdata[, -which(names(testdata) == response_var)]
 y_test <- testdata[, which(names(testdata) == response_var)]
 #===============================================================================
 # Creating the control, and grid parameters
 #===============================================================================
 library(mlr)
 control <- trainControl(method = "cv", number=10, classProbs = TRUE)
 control
 grid = expand.grid(.mtry = 4)
 grid
 resample = makeResampleDesc("CV", iters = 3L)
 
 #-----------------------------------------------------------------
 #Random forest using traindata. 
 library(randomForest)
 library(rpart)
 library(rpart.plot)
 library(ROCR)
 library(caretEnsemble)
 
 startTime <- Sys.time()
 rfModel = randomForest( legitimate~., 
                         data= traindata,
                         mtry= 4, ntree=501, 
                         importance = TRUE) 
 rfModel 
 tail(plot(rfModel))
 #plot(rfModel, main = "Random Forest Model graph", legend('topright', legend=c('Malicious', 'OOB', 'Not malicious'), col=c('red', 'Black', 'Green'), lty = 1))
 endTime <- Sys.time()
 endTime - startTime #Time difference of 1.00824 mins.
 
 # predict 
 rf_predict = predict(rfModel,newdata= data.frame(x_test)) # it run
 rf_predict
 ## performance evaluation
 rfPR = postResample(pred=rf_predict, obs= y_test) # it run
 rfPR
 
 #------------# Calculate the ROC curve and AUC for RF model--------------------------
 # Convert binary factor to numeric vector
 library(pROC)
 y_test_numeric <- as.numeric(as.character(y_test))
 rf_predict_numeric <- as.numeric(as.character(rf_predict))
 roc_dataRF <- roc(y_test_numeric, rf_predict_numeric)
 auc <- auc(roc_dataRF)
 auc # shows the value of AUC. It is 0.9936
 #barplot(table(traindata$legitimate)) # Error in plot.new() : figure margins too large
 
 #-------------confusion_matrix for RF model---------------------------------------
library(caret)
 #confusionMatrixRF <- confusionMatrix(rf_predict, y_test)
 #print(confusionMatrixRF)
# print(confusionMatrixRF$table) # it shows a count of predictions per option of 
 #the outcome Var.
 
 
 #------------- Plotting ROC for RF -------------------------------
 
 ggroc(roc_dataRF, legacy.axes = TRUE, color = "purple") +
   geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "black") +
   labs(title = paste("Random Forest B ROC 
       (AUC =", round(auc, 2), ")"),
        #subtitle = svm.subtitle,
        x = "Specificity",
        y = "Sensitivity",
        color = "SVM") +
   theme_bw()
 
 
 library(caret)
 
 # Creating a plot between Characteristics and legitimate.
 ggplot(traindata, aes( Characteristics, legitimate )) +
   geom_point(colour = "red") + 
   #geom_errorbarh(height=.2) +
   labs(title='Title Of Plot', x='Predictor', y = ' Outcome')
 
 #-----------------Tuning------------------------
 
 set.seed(568)
 #fit the random forest model
 malwareModel <- randomForest(formula = legitimate ~ ., data = traindata)
 #display fitted model
 malwareModel
 #find number of trees that produce lowest test MSE
 which.min (malwreModel$mse)
 #find RMSE of best model
 sqrt(malwareModel$mse[which.min(model$mse)])
 #plot the model
 Plot(malwareModel)
 #produce variable importance plot
 varImpPlot(malwareModel) 
 #Step 3: Tune the Model
 #By default, the randomForest() function uses 501 trees and (total predictors/3)
 #randomly selected predictors as potential candidates at each split.
 #We can adjust these parameters by using the tuneRF() function.
 model_tuned <- tuneRF(
   x= ImportVar10[,-11], #define predictor variables
   y= ImportVar10$legitimate, #define response variable
   ntreeTry=500,
   mtryStart=4, 
   stepFactor=1.5,
   improve=0.01,
   trace=FALSE #don't show real-time progress
 )
 
 model_tuned
 