# Load Data
## First thing run will be to set the working directory to my file location
## Next some libraries will be loaded along with the dataset
library(AppliedPredictiveModeling)
library(caret)
library(e1071)
library(corrplot)
library(lattice)
library(dplyr)

# Load dataset for your machine 
# Set your working directory to the directory to where all project R Scripts 
cwd <- getwd()
file.name <- "Kaggle-data-FINAL.csv"
paste(cwd, file.name ,sep="/")
df <- read.csv(paste(cwd, file.name ,sep="/"))

# Label the response variable column name
response_var <- "legitimate" 
## Confirm there are no nulls
sum(is.na(df))
## Check for near zero variance using the nearZeroVar function
zeroVar <- nearZeroVar(df, saveMetrics=FALSE)
## Column index 10, 30, 31, 32, 33, and 47 all have zero  variance
colnames(df[c(10,30,31,32,33,47)])
##Dropping near zero variance columns
df_trans1 <- df[, -c(10,30,31,32,33,47)] 
##Check to see what class each attribute is
sapply(df_trans1, class)
##Can see that md5 is a character class so we will drop this as it is not needed also will drop id column
df_trans2 <- df_trans1[,-c(1,2)]
colnames(df_trans2)

## Find highly correlated variables; remove corr magnitude .8 or higher
corr_matrix <- cor(df_trans2)
high_corr <- findCorrelation(corr_matrix, cutoff = 0.8)
removed_variables <- names(df_trans2)[high_corr]
removed_variables

df_trans3 <- df_trans2[, -high_corr]
dim(df_trans3)
head(df_trans3)
## Down to 41 variables including the outcome variable
## Next step will be to standardrize the variables
## https://www.r-bloggers.com/2022/07/how-to-standardize-data-in-r/
df_stdz <- df_trans3 %>%
  mutate_at(-which(names(df_trans3) == response_var), ~(scale(.) %>% 
                                                          as.vector))
# Matts normalization
df_stdz <- df_trans3 %>%
  mutate_at(-which(names(df_trans3) == response_var), ~( (.-min(.))/(max(.)-min(.)) ))

# df_stdz <- mutate_all(df_stdz, function(x) as.numeric(as.character(x)))

## Now that all variables have been standardized, a test train split will be created
## I will be using a 70/30 split
set.seed(568)
# Create the partition index with a 70/30 split
index <- createDataPartition(y = df_stdz$legitimate, p = 0.7, list = FALSE)

# Create the train and test sets
train <- df_stdz[index, ]
test <- df_stdz[-index, ]
## Create the df with X and Y variables
# Remove "legitimate" column from training and test datasets
x_train <- train[, -which(names(train) == response_var)]
y_train <- train[, which(names(train) == response_var)]
y_train <- factor(y_train)
x_test <- test[, -which(names(test) == response_var)]
y_test <- test[, which(names(test) == response_var)]

## Set up the control option
# control <- rfeControl(functions = lrFuncs,
#                       method = "repeatedcv", ## Repeated CV
#                       repeats = 5, ## Number of repeats
#                       number = 10, ## Number of folds
#                       verbose = FALSE
## Set up the control option

control <- rfeControl(functions = lrFuncs,
                      method = "repeatedcv", ## Repeated CV
                      repeats = 2,
                      number = 3, ## Number of folds
                      verbose = FALSE
)
result_rfe1 <- rfe(x = x_train,
                   y = y_train,
                   sizes = c(1:20), # sizes = c(1:ncol(x_train)),
                   rfeControl = control)

result_rfe1
summary(result_rfe1)
# END:   Model Creation
#===============================================================================
# BEGIN: Plot Predictor Strength
library(ggplot2)

# Create Substring for Model Data
subtitle_string <- paste(
  "Method", control$method,
  "with", control$repeats,"Repeat(s)",
  "using", control$number, "Fold(s)"
  , sep = " "
)

# Extract variable importance from result_rfe1
var_importance <- varImp(result_rfe1)

# Create a data frame with variable names and importance scores
var_df <- data.frame(variable = row.names(var_importance),
                     importance = var_importance$Overall)

# Convert the variable column to a factor with ordered levels in reverse order
var_df$variable <- factor(var_df$variable, levels = rev(var_df$variable))

# Sort the data frame by importance scores in descending order
var_df <- var_df[order(var_df$importance, decreasing = TRUE),]

# Create a bar plot of variable importance in descending order
ggplot(var_df, aes(x = variable, y = importance)) + 
  geom_bar(stat = "identity", color="black", fill = "forestgreen") +
  labs(title = "Variable Importance Plot",
       subtitle = subtitle_string,
       xlab("Predictor"), ylab("Importance Score")) +
  xlab("Predictor") + ylab("Importance Score") +
  coord_flip() +
  theme(axis.text.x = element_text(angle = 0, vjust = 0.5, hjust = 1))

## First build a logistic model using all 41 variables
set.seed(568)
ctrl <- trainControl(method = "cv",
                     number = 5,
                     classProbs=TRUE,
                     savePredictions=TRUE)
lr.model <- train(x = x_train, y = make.names(y_train), method = "glm",
                  family = binomial,
                   trControl=ctrl)
lr.model
varImp2 <- varImp(lr.model)
# Create a data frame with variable names and importance scores
var_df2 <- data.frame(variable = row.names(varImp2),
                     importance = varImp2$Overall)

# Convert the variable column to a factor with ordered levels in reverse order
var_df2$variable <- factor(var_df2$variable, levels = rev(var_df2$variable))

# Sort the data frame by importance scores in descending order
var_df2 <- var_df2[order(var_df2$importance, decreasing = TRUE),]
## Plot these variable importance
ggplot(varImp2, aes(x = variable, y = importance)) + 
  geom_bar(stat = "identity", color="black", fill = "forestgreen") +
  labs(title = "Variable Importance Plot",
       subtitle = "Method Using 40 Predictos with 5 Folds",
       xlab("Predictor"), ylab("Importance Score")) +
  xlab("Predictor") + ylab("Importance Score") +
  coord_flip() +
  theme(axis.text.x = element_text(angle = 0, vjust = 0.5, hjust = 1))
## Apply model on test set
predictions1 <- predict(lr.model, newdata = test)
predictions1
y_test <- as.factor(y_test)
y_test <- make.names(y_test)
y_test
## Create confusion matrix
CM1 <- confusionMatrix(data = predictions1, as.factor(y_test))
CM1
## Next I will create a new model using the 20 variables from the RFE
top_8_rfe <- row.names(var_importance)[0:20]
set.seed(568)
ctrl2 <- trainControl(method = "cv",
                     number = 10,
                     classProbs=TRUE,
                     savePredictions=TRUE)
lr.model2 <- train(x = x_train[top_8_rfe], y = make.names(y_train), method = "glm",
                  family = binomial,
                  trControl=ctrl)
varImp3 <- varImp(lr.model2)
## Plot these variable importance
ggplot(varImp3, aes(x = variable, y = importance)) + 
  geom_bar(stat = "identity", color="black", fill = "forestgreen") +
  labs(title = "Variable Importance Plot",
       subtitle = "Method Using 20 Predictors from RFE with 10 Folds",
       xlab("Predictor"), ylab("Importance Score")) +
  xlab("Predictor") + ylab("Importance Score") +
  coord_flip() +
  theme(axis.text.x = element_text(angle = 0, vjust = 0.5, hjust = 1))
## Apply model on test set
predictions2 <- predict(lr.model2, newdata = test)
levels(predictions2)
## Create confusion matrix
CM2 <- confusionMatrix(data = predictions2, as.factor(y_test))
CM2
