##################################################################
##################################################################
#preprocessing for Random Forest model A
##################################################################

# Load dataset
# Set your working directory to the directory to where all project R Scripts 
cwd <- getwd()
file.name <- "Kaggle-data-FINAL.csv"
paste(cwd, file.name ,sep="/")
df <- read.csv(paste(cwd, file.name ,sep="/"))


# Label the response variable column name
response_var <- "legitimate" 
## Confirm there are no nulls
sum(is.na(df))

## Check for near zero variance using the nearZeroVar function
zeroVar <- caret::nearZeroVar(df, saveMetrics=FALSE)
## Column index 10, 30, 31, 32, 33, and 47 all have zero  variance
colnames(df[c(10,30,31,32,33,47)])
##Dropping near zero variance columns
df_trans1 <- df[, -c(10,30,31,32,33,47)] 
##Check to see what class each attribute is
sapply(df_trans1, class)
##Can see that md5 is a character class so we will drop this as it is not needed
df_trans2 <- df_trans1[,-2]
colnames(df_trans2)

## Find highly correlated variables; remove corr magnitude .8 or higher
corr_matrix <- cor(df_trans2)
high_corr <- caret::findCorrelation(corr_matrix, cutoff = 0.8)
removed_variables <- names(df_trans2)[high_corr]
removed_variables

df_trans3 <- df_trans2[, -high_corr]
dim(df_trans3)
head(df_trans3)


## Now that all variables have been standardized, a test train split will be created
## I will be using a 70/30 split
set.seed(568)
# Create the partition index with a 70/30 split
index <- caret::createDataPartition(y = df_stdz$legitimate, p = 0.7, list = FALSE)

# Create the train and test sets
train <- df_stdz[index, ]
test <- df_stdz[-index, ]
## Create the df with X and Y variables
# Remove "legitimate" column from training and test datasets
x_train <- train[, -which(names(train) == response_var)]
y_train <- train[, which(names(train) == response_var)]
y_train <- factor(y_train)
x_test <- test[, -which(names(test) == response_var)]
y_test <- test[, which(names(test) == response_var)]




######################
######################
######################
#random forest model A
#uses all 40 dependent variables
######################

library(randomForest)
#
set.seed(568)
startTime <- Sys.time()
rfModel = randomForest( y_train ~ .-ID, data=x_train, ntree=600 )
endTime <- Sys.time()
endTime - startTime

rfModel
plot(rfModel)


#Train evaluation
rf_yHat_train = predict(rfModel,newdata=x_train)
rfModelTrainResults = caret::postResample(pred=rf_yHat_train, obs=y_train)

rfModelTrainResults

# predict solubility:
rf_yHat = predict(rfModel,newdata=x_test)

## performance evaluation
rfPR = caret::postResample(pred=rf_yHat, obs=as.factor(y_test))
rfPR


#ROC
#value of 2 here predicts legit=1, change it to a 1 and it predicts legit=0
rand.preds.final <- predict(rfModel, x_test, type="prob")[, 2]
rand.roc.final <- pROC::roc(y_test, rand.preds.final)
print(rand.roc.final)

plot(rand.roc.final, main="Random Forest A ROC \n AUC = 0.9993")

#Confusion Matrix
final_rand_test = predict(rfModel,newdata=x_test, type='class')
caret::confusionMatrix(final_rand_test, as.factor(y_test))


varImpRand <- caret::varImp(rfModel)
varImpRand
# Create a data frame with variable names and importance scores
var_df_rand <- data.frame(variable = row.names(varImpRand),
                          importance = varImpRand$Overall)

# Convert the variable column to a factor with ordered levels in reverse order
var_df_rand$variable <- factor(var_df_rand$variable, levels = rev(var_df_rand$variable))

# Sort the data frame by importance scores in descending order
var_df_rand <- var_df_rand[order(var_df_rand$importance, decreasing = TRUE),]

ggplot(var_df_rand, aes(x = reorder(variable, +importance), y= importance)) + 
  geom_bar(stat = "identity", color="black", fill = "forest green") +
  labs(title = "Variable Importance Plot",
       subtitle = "Random Forest with ntree = 600",
       xlab("Predictor"), ylab("Importance Score")) +
  xlab("Predictor") + ylab("Importance Score") +
  coord_flip() +
  theme(axis.text.x = element_text(angle = 0, vjust = 0.5, hjust = 1))


plot(getTree(rfModel, 3, labelVar=TRUE))
