# Load Data
## First thing run will be to set the working directory to my file location
## Next some libraries will be loaded along with the dataset
# Load the necessary library
library(tidyverse)
library(neuralnet)
library(dplyr)
library(caret)
library(moments)
library(ROCR)
library(RSNNS)
#===============================================================================
# Load dataset for your machine 
#===============================================================================
# Set your working directory to the directory to where all project R Scripts
cwd <- getwd()
file.name <- "Kaggle-data-FINAL.csv"
paste(cwd, file.name ,sep="/")
df <- read.csv(paste(cwd, file.name ,sep="/"))
data.load <- df
#===============================================================================
# Pre-Process Data
# Label the response variable column name
response_var <- "legitimate" 
## Confirm there are no nulls
sum(is.na(df))
## Check for near zero variance using the nearZeroVar function
zeroVar <- nearZeroVar(df, saveMetrics=FALSE)
## Column index 10, 30, 31, 32, 33, and 47 all have zero  variance
zeroVar.names <- colnames(df[zeroVar])
trans1 <- (paste("ZeroVar column removed:", zeroVar.names, collapse = "\n"))
##Dropping near zero variance columns
df_trans1 <- df[, -zeroVar] 
##Drop the ID column because this is an index and is non-informational as well as the md5 column
id.columns <- colnames(df_trans1[1:2])
df_trans2 <- select(df_trans1, -id.columns)
trans2 <- (paste("ID column removed:", id.columns, collapse = "\n"))
## Find highly correlated variables; remove corr magnitude .8 or higher
corr_matrix <- cor(df_trans2)
high_corr <- findCorrelation(corr_matrix, cutoff = 0.8)
high.corr.names <- names(df_trans2)[high_corr]
trans3 <- (paste("High Correlation removed:", high.corr.names, collapse = "\n"))

##Remove highly correlated variables from data frame
df_trans3 <- df_trans2[, -high_corr]
##Convert outcome variable to 2-level factor
df_trans3$legitimate <- as.factor(df_trans3$legitimate)

# Removal Recap
df.preprocessed <- df_trans3
cat(trans1, trans2, trans3, sep="\n")
#===============================================================================
# Split Test Train Data
set.seed(568)
index <- createDataPartition(y=df.preprocessed$legitimate, p = 0.7, list = FALSE)
train <- df.preprocessed[index, ]
test <- df.preprocessed[-index, ]
x_train <- train[, -which(names(train) == response_var)]
y_train <- train[, which(names(train) == response_var)]
x_test <- test[, -which(names(test) == response_var)]
y_test <- test[, which(names(test) == response_var)]
#===============================================================================
# Pre-Process Data (additional)
#===============================================================================
# # remove non numeric data
# library(dplyr)
# data <- data %>% select_if(~!is.character(.))
# print(paste("Removed non-numeric column: ",
#             colnames(data.load %>% select_if(~is.character(.)))))
# # remove near zero variance
# library(caret)
# nzv.index <- nearZeroVar(data)
# nzv.colnames <- colnames(data[nzv.index])
# data <- data[, -which(names(data) %in% nzv.colnames)]
# print(paste("Removed near-zero variance column: ", nzv.colnames))
# # remove skewed columns
# library(moments)
# skew <- apply(data, 2, skewness)
# threshold <- 1 # Set your desired skewness threshold
# skewed_cols <- names(data)[skew > threshold]
# data <- select(data, -skewed_cols)
# # remove feature specific vars (ID, machine, etc...)
# targeted.removal <- c("ID")
# data <- data[, -which(names(data) %in% targeted.removal)]
# # > colnames(data)
# # [1] "DllCharacteristics"     "SectionsMeanEntropy"    "SectionsMinEntropy"     "SectionsMaxEntropy"     "ResourcesMeanEntropy"
# # [6] "ResourcesMinEntropy"    "ResourcesMaxEntropy"    "VersionInformationSize" "legitimate"
#===============================================================================
# Split Test Train Data
#===============================================================================
# train_idx <- sample(1:nrow(data), size = round(0.8*nrow(data)), replace = FALSE)
# train_data <- data[train_idx, ]
# test_data <- data[-train_idx, ]

#===============================================================================
# Create Neural Net
#===============================================================================
# Define the formula with a sigmoid activation function
formula <- as.formula(paste("legitimate ~", paste(colnames(x_train), collapse = "+")))
act.fct <- "logistic"
library(nnet)

# Define the neural network model with a sigmoid activation function
nn_model <- nnet(formula = formula, data = train, size = 5, 
                 maxit = 1000, linout = FALSE, 
                 trace = FALSE, MaxNWts = 100000, 
                 abstol = 1e-8, 
                 rang = 0.1, decay = 0.01, 
                 skip = FALSE, 
                 Hess = TRUE,
                 act.fct = act.fct)

# Predict the class probabilities for the test data
nn_prob <- predict(nn_model, newdata = test, type = "class.probs")

# Predict the class labels for the test data
library(caret)
nn_pred <- ifelse(nn_prob[,1] > 0.5, "B", "M")
# Create a confusion matrix
confusionMatrix(nn_pred, y_test)

#===============================================================================
# Define the formula with a sigmoid activation function
formula <- as.formula(paste("legitimate ~", paste(colnames(x_train), collapse = "+")))
act.fct <- "logistic"
# Create the neural network model with a sigmoid activation function
model <- neuralnet(formula, data = train, 
                   hidden = 3, threshold = 0.10, stepmax = 100, act.fct = act.fct)

# Make predictions on the testing set
predictions <- predict(model, test)
# Convert the predictions to binary labels
binary_predictions <- ifelse(predictions > 0.5, 1, 0)
# Calculate the accuracy
accuracy <- sum(binary_predictions == y_test) / nrow(y_test)
# Print the accuracy
cat("Accuracy:", accuracy)


# Make predictions on the testing set
predictions <- predict(model, test_data[, -1])
# Convert the predictions to binary labels
binary_predictions <- ifelse(predictions > 0.5, 1, 0)

# Create the confusion matrix
confusion_matrix <- table(test[,"legitimate"], binary_predictions)

# Print the confusion matrix
print(confusion_matrix)


# Calculate precision
precision <- confusion_matrix[2,2] / sum(confusion_matrix[,2])
cat("Precision:", precision, "\n")

# Calculate recall
recall <- confusion_matrix[2,2] / sum(confusion_matrix[2,])
cat("Recall:", recall, "\n")

#===============================================================================
# Useful Visualizations
#===============================================================================

#===============================================================================
# Useful Visualizations
#===============================================================================
# Load the ROCR package
library(ROCR)
# Make predictions on the testing set
predictions <- predict(model, test_data[, -1])
pred <- prediction(predictions, test_data$legitimate)
# Calculate the AUC score
auc <- performance(pred, "auc")@y.values[[1]]
cat("AUC score:", auc, "\n")
# Calculate the ROC curve
perf <- performance(pred, "tpr", "fpr")
{
plot(perf, main="ROC Curve", col="blue", lwd=3, legacy.axes=TRUE)
abline(a=0, b=1, lty=3, col="gray")
}



#-------------------------------------------------------------------------------
# Plot the learning curves
learning_curves <- neuralnet(formula, data = train_data, hidden = 3, 
                             threshold = 0.01, act.fct = act.fct, 
                             linear.output = FALSE, lifesign = "full", 
                             stepmax = 1e7)
plot(learning_curves, rep="best", main="Learning Curves")

library(ggplot2)

# Train the neural network model and store the learning curve information
learning_curves <- neuralnet(formula, data = train_data, hidden = 3, 
                             threshold = 0.01, act.fct = act.fct, 
                             linear.output = FALSE, lifesign = "full", 
                             stepmax = 1e7)
# Extract the learning curve data from the neural network model
learning_curve_data <- data.frame(epoch = 1:nrow(learning_curves$result.matrix),
                                  error = learning_curves$result.matrix[,1])

ggplot(data = learning_curve_data, aes(x = epoch, y = error)) +
  geom_line() +
  geom_point() +
  labs(x = "Epoch", y = "Error", title = "Learning Curves") +
  theme_bw()

#-------------------------------------------------------------------------------
# Plot the weight histograms
library(RSNNS)
library(ggplot2)

# Extract the weights from the neural network model
weights <- weights(model)

# Convert the weights to a data frame
df_weights <- data.frame(unlist(weights))

# Plot the weight histogram using ggplot2
ggplot(df_weights, aes(x = unlist(weights))) + 
  geom_histogram(bins = 50) +
  labs(x = "Weight", y = "Frequency", title = "Histogram of Weights") +
  theme_bw()
#-------------------------------------------------------------------------------
# Plot the hidden layer activation histograms
library(neuralnet)
library(ggplot2)

# Compute the activation values for the first hidden layer
hidden1_act <- compute(model, test_data[, -1])$net.result[, 1]

# Plot the histograms of the activation values
ggplot(data = data.frame(hidden1_act), aes(x = hidden1_act)) +
  geom_histogram(bins = 20) +
  labs(x = "Activation Value", y = "Frequency", 
       title = "Histogram of Activation Values for First Hidden Layer")

#-------------------------------------------------------------------------------
# Plot the heatmap of inputs and weights
heatmap_data <- train_data[, -1]
heatmap_data[,ncol(heatmap_data)+1] <- model$net.result[[1]][,1]
heatmap_data <- cor(heatmap_data)
corrplot(heatmap_data, method="color", type="upper", order="hclust", 
         tl.cex=0.8, addrect=3, rect.col="white")
