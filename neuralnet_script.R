# Load Data
## First thing run will be to set the working directory to my file location
## Next some libraries will be loaded along with the dataset
# Load the necessary library
library(tidyverse)
library(neuralnet)
library(dplyr)
library(caret)
library(moments)
library(ROCR)
library(RSNNS)
#===============================================================================
# Load dataset for your machine 
#===============================================================================
# Set your working directory to the directory to where all project R Scripts 
cwd <- getwd()
file.name <- "Kaggle-data-FINAL.csv"
paste(cwd, file.name ,sep="/")
data <- read.csv(paste(cwd, file.name ,sep="/"))
data.load <- data
set.seed(568)
#===============================================================================
# Pre-Process Data
#===============================================================================

# remove non numeric data
library(dplyr)
data <- data %>% select_if(~!is.character(.))
print(paste("Removed non-numeric column: ", 
            colnames(data.load %>% select_if(~is.character(.)))))

# remove near zero variance
library(caret)
nzv.index <- nearZeroVar(data)
nzv.colnames <- colnames(data[nzv.index])
data <- data[, -which(names(data) %in% nzv.colnames)]
print(paste("Removed near-zero variance column: ", nzv.colnames))

# remove skewed columns
library(moments)
skew <- apply(data, 2, skewness)
threshold <- 1 # Set your desired skewness threshold
skewed_cols <- names(data)[skew > threshold]
data <- select(data, -skewed_cols)


# remove feature specific vars (ID, machine, etc...)
targeted.removal <- c("ID")
data <- data[, -which(names(data) %in% targeted.removal)]

# > colnames(data)
# [1] "DllCharacteristics"     "SectionsMeanEntropy"    "SectionsMinEntropy"     "SectionsMaxEntropy"     "ResourcesMeanEntropy"  
# [6] "ResourcesMinEntropy"    "ResourcesMaxEntropy"    "VersionInformationSize" "legitimate"

#===============================================================================
# Split Test Train Data
#===============================================================================
train_idx <- sample(1:nrow(data), size = round(0.8*nrow(data)), replace = FALSE)
train_data <- data[train_idx, ]
test_data <- data[-train_idx, ]

#===============================================================================
# Create Neural Net
#===============================================================================
# Define the formula with a sigmoid activation function
formula <- as.formula(paste("legitimate ~", paste(colnames(data)[-1], collapse = "+")))
act.fct <- "logistic"
# Create the neural network model with a sigmoid activation function
model <- neuralnet(formula, data = train_data, 
                   hidden = 3, threshold = 0.01, act.fct = act.fct)

# Make predictions on the testing set
predictions <- predict(model, test_data[, -1])
# Convert the predictions to binary labels
binary_predictions <- ifelse(predictions > 0.5, 1, 0)
# Calculate the accuracy
accuracy <- sum(binary_predictions == test_data[, "legitimate"]) / nrow(test_data)
# Print the accuracy
cat("Accuracy:", accuracy)


# Make predictions on the testing set
predictions <- predict(model, test_data[, -1])
# Convert the predictions to binary labels
binary_predictions <- ifelse(predictions > 0.5, 1, 0)

# Create the confusion matrix
confusion_matrix <- table(test_data[, "legitimate"], binary_predictions)

# Print the confusion matrix
print(confusion_matrix)


# Calculate precision
precision <- confusion_matrix[2,2] / sum(confusion_matrix[,2])
cat("Precision:", precision, "\n")

# Calculate recall
recall <- confusion_matrix[2,2] / sum(confusion_matrix[2,])
cat("Recall:", recall, "\n")

#===============================================================================
# Useful Visualizations
#===============================================================================

#===============================================================================
# Useful Visualizations
#===============================================================================
# Load the ROCR package
library(ROCR)
# Make predictions on the testing set
predictions <- predict(model, test_data[, -1])
pred <- prediction(predictions, test_data$legitimate)
# Calculate the AUC score
auc <- performance(pred, "auc")@y.values[[1]]
cat("AUC score:", auc, "\n")
# Calculate the ROC curve
perf <- performance(pred, "tpr", "fpr")
{
plot(perf, main="ROC Curve", col="blue", lwd=3, legacy.axes=TRUE)
abline(a=0, b=1, lty=3, col="gray")
}



#-------------------------------------------------------------------------------
# Plot the learning curves
learning_curves <- neuralnet(formula, data = train_data, hidden = 3, 
                             threshold = 0.01, act.fct = act.fct, 
                             linear.output = FALSE, lifesign = "full", 
                             stepmax = 1e7)
plot(learning_curves, rep="best", main="Learning Curves")

library(ggplot2)

# Train the neural network model and store the learning curve information
learning_curves <- neuralnet(formula, data = train_data, hidden = 3, 
                             threshold = 0.01, act.fct = act.fct, 
                             linear.output = FALSE, lifesign = "full", 
                             stepmax = 1e7)
# Extract the learning curve data from the neural network model
learning_curve_data <- data.frame(epoch = 1:nrow(learning_curves$result.matrix),
                                  error = learning_curves$result.matrix[,1])

ggplot(data = learning_curve_data, aes(x = epoch, y = error)) +
  geom_line() +
  geom_point() +
  labs(x = "Epoch", y = "Error", title = "Learning Curves") +
  theme_bw()

#-------------------------------------------------------------------------------
# Plot the weight histograms
library(RSNNS)
library(ggplot2)

# Extract the weights from the neural network model
weights <- weights(model)

# Convert the weights to a data frame
df_weights <- data.frame(unlist(weights))

# Plot the weight histogram using ggplot2
ggplot(df_weights, aes(x = unlist(weights))) + 
  geom_histogram(bins = 50) +
  labs(x = "Weight", y = "Frequency", title = "Histogram of Weights") +
  theme_bw()
#-------------------------------------------------------------------------------
# Plot the hidden layer activation histograms
library(neuralnet)
library(ggplot2)

# Compute the activation values for the first hidden layer
hidden1_act <- compute(model, test_data[, -1])$net.result[, 1]

# Plot the histograms of the activation values
ggplot(data = data.frame(hidden1_act), aes(x = hidden1_act)) +
  geom_histogram(bins = 20) +
  labs(x = "Activation Value", y = "Frequency", 
       title = "Histogram of Activation Values for First Hidden Layer")

#-------------------------------------------------------------------------------
# Plot the heatmap of inputs and weights
heatmap_data <- train_data[, -1]
heatmap_data[,ncol(heatmap_data)+1] <- model$net.result[[1]][,1]
heatmap_data <- cor(heatmap_data)
corrplot(heatmap_data, method="color", type="upper", order="hclust", 
         tl.cex=0.8, addrect=3, rect.col="white")
