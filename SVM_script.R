#===============================================================================
# Load dependencies/modules/libraries
#===============================================================================
library(dplyr)
library(ggplot2)
library(caret)
library(corrplot)
library(lattice)
library(e1071)
library(ggpubr)
library(pROC)
#===============================================================================
# Load dataset for your machine 
#===============================================================================
# Set your working directory to the directory where all project R Scripts
cwd <- getwd()
file.name <- "Kaggle-data-FINAL.csv"
paste(cwd, file.name ,sep="/")
df <- read.csv(paste(cwd, file.name ,sep="/"))
data.load <- df
#===============================================================================
# Pre-Process Data
# Label the response variable column name
response_var <- "legitimate" 
## Confirm there are no nulls
sum(is.na(df))
## Check for near zero variance using the nearZeroVar function
zeroVar <- nearZeroVar(df, saveMetrics=FALSE)
## Column index 10, 30, 31, 32, 33, and 47 all have zero  variance
zeroVar.names <- colnames(df[zeroVar])
trans1 <- (paste("ZeroVar column removed:", zeroVar.names, collapse = "\n"))
##Dropping near zero variance columns
df_trans1 <- df[, -zeroVar] 
##Drop the ID column because this is an index and is non-informational as well as the md5 column
id.columns <- colnames(df_trans1[1:2])
df_trans2 <- select(df_trans1, -id.columns)
trans2 <- (paste("ID column removed:", id.columns, collapse = "\n"))
## Find highly correlated variables; remove corr magnitude .8 or higher
corr_matrix <- cor(df_trans2)
high_corr <- findCorrelation(corr_matrix, cutoff = 0.8)
high.corr.names <- names(df_trans2)[high_corr]
trans3 <- (paste("High Correlation removed:", high.corr.names, collapse = "\n"))

##Remove highly correlated variables from data frame
df_trans3 <- df_trans2[, -high_corr]
##Convert outcome variable to 2-level factor
df_trans3$legitimate <- as.factor(df_trans3$legitimate)

# Removal Recap
df.preprocessed <- df_trans3
cat(trans1, trans2, trans3, sep="\n")
file.export <- paste(cwd,"preprocessed_data.csv",sep="/")
write.csv(df.preprocessed, file.export, row.names=FALSE)
#===============================================================================
# Split Test Train Data
set.seed(568)
#index <- createDataPartition(y=df.preprocessed$legitimate, p = 0.7, list = FALSE)
# Intentionally reduce training set to 10% instead of 70% or 80%
index <- createDataPartition(y=df.preprocessed$legitimate, p = 0.1, list = FALSE)
train <- df.preprocessed[index, ]
test <- df.preprocessed[-index, ]
x_train <- train[, -which(names(train) == response_var)]
y_train <- train[, which(names(train) == response_var)]
x_test <- test[, -which(names(test) == response_var)]
y_test <- test[, which(names(test) == response_var)]
#===============================================================================
# Create Model Simple SVM on 10% training data
#===============================================================================
svm_linear <- svm(legitimate ~ ., data = train, kernel = "linear", cost = 10)
svm_predictions <- predict(svm_linear, x_test, decision.values = TRUE)
confusion_matrix <- confusionMatrix(svm_predictions, y_test)

# accuracy <- sum(svm_predictions == y_test) / length(y_test)
# print(paste("Accuracy:", round(accuracy * 100, 2), "%"))
# Create confusion matrix
# confusion_matrix <- confusionMatrix(svm_predictions, y_test)
svm.accuracy <- confusion_matrix$overall["Accuracy"]
svm.kappa <- confusion_matrix$overall["Kappa"]
# Print the confusion matrix
print(confusion_matrix$table)
# Calculate the ROC curve and AUC
library(pROC)
# Convert binary factor to numeric vector
y_test_numeric <- as.numeric(as.character(y_test))
svm_predictions_numeric <- as.numeric(as.character(svm_predictions))
roc_data <- roc(y_test_numeric, svm_predictions_numeric)
auc <- auc(roc_data)

svm.subtitle <- paste("Accuracy: ", round(svm.accuracy, 4),
                      " Kappa: ", round(svm.kappa, 4))
# Plot the ROC curve using ggplot2
library(ggplot2)
# if(!require(devtools)) install.packages("devtools")
# devtools::install_github("kassambara/ggpubr")
install.packages("ggpubr")
library(ggpubr)
library(pROC)
ggroc(roc_data, legacy.axes = TRUE, color = "blue") +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "black") +
  labs(title = paste("ROC Curve for SVM (AUC =", round(auc, 2), ")"),
       subtitle = svm.subtitle,
       x = "False Positive Rate (1 - Specificity)",
       y = "True Positive Rate (Sensitivity)",
       color = "SVM") +
  theme_bw()
#===============================================================================
# Kernel Function Comparisons

# Train and evaluate SVM models with different kernels
svm_linear <- svm(legitimate ~ ., data = train, kernel = "linear", cost = 1)
svm_poly <- svm(legitimate ~ ., data = train, kernel = "polynomial", degree = 4, coef0 = 1)
svm_rbf <- svm(legitimate ~ ., data = train, kernel = "radial", gamma = 0.2)
svm_sigmoid <- svm(legitimate ~ ., data = train, kernel = "sigmoid", gamma = 0.1, coef0 = 1)
#-------------------------------------------------------------------------------
library(caret)
# Define the training control
ctrl <- trainControl(method = "cv", number = 10)
# Train and evaluate the models
set.seed(568) # for reproducibility


{# Linear Model Timed
linear.start <- Sys.time()
svm_linear <- svm(legitimate ~ ., data = train, kernel = "linear", cost = 1)
linear.end <- Sys.time()
linear.time <- linear.end - linear.start
linear.time
}
{ # Linear Model Metrics
  library(pROC)
  library(caret)
  # Create predictions on the test set
  svm_linear_pred <- predict(svm_linear, newdata = test)
  # Create the confusion matrix
  confusionMatrix(data = svm_linear_pred, reference = test$legitimate)
  # Extract the accuracy and kappa
  accuracy <- confusionMatrix(data = svm_linear_pred, reference = test$legitimate)$overall['Accuracy']
  kappa <- confusionMatrix(data = svm_linear_pred, reference = test$legitimate)$overall['Kappa']
  # Extract the AUC
  svm_linear_pred_num <- as.numeric(svm_linear_pred) - 1
  svm_linear_auc <- roc(test$legitimate, svm_linear_pred_num)$auc
  cat("Acc:", accuracy, " Kappa:", kappa, " AUC:", svm_linear_auc)
}

{ # Polynomial Model Timed
poly.start <- Sys.time()
svm_poly <- svm(legitimate ~ ., data = train, kernel = "polynomial", degree = 4, coef0 = 1)
poly.end <- Sys.time()
poly.time <- poly.end - poly.start
poly.time
}
{# Polynomial Metrics
  library(caret)
  # Create predictions on the test set
  svm_poly_pred <- predict(svm_poly, newdata = test)
  # Create the confusion matrix
  confusionMatrix(data = svm_poly_pred, reference = test$legitimate)
  # Calculate the accuracy and kappa
  svm_poly_acc <- confusionMatrix(data = svm_poly_pred, reference = test$legitimate)$overall[1]
  svm_poly_kappa <- confusionMatrix(data = svm_poly_pred, reference = test$legitimate)$overall[2]
  # Calculate the AUC
  svm_poly_pred_num <- as.numeric(svm_poly_pred) - 1
  svm_poly_auc <- roc(test$legitimate, predict(svm_poly, newdata = test, decision.values = FALSE))$auc
  # Print the results
  cat("Accuracy:", svm_poly_acc, "\n")
  cat("Kappa:", svm_poly_kappa, "\n")
  cat("AUC:", svm_poly_auc, "\n")
}
{
  library(caret)
  library(ggplot2)
  # Train the SVM model with polynomial kernel
  svm_poly <- train(legitimate ~ ., data = train, method = "svmPoly",
                    trControl = trainControl(method = "cv", number = 10))
  # Compute the feature importance using permutation-based method
  importance <- varImp(svm_poly, scale = FALSE, useModel = FALSE)
  # Create a ggplot of the variable importance
  ggplot(importance, aes(x = Reorder(row.names(importance), -Overall), y = Overall)) +
    geom_bar(stat = "identity", fill = "steelblue") +
    labs(x = "Features", y = "Importance", title = "Variable Importance Plot",
         subtitle = "Full SVM ~ Polynomial")
}

{# RBF Model Timed
rbf.start <- Sys.time()
svm_rbf <- svm(legitimate ~ ., data = train, kernel = "radial", gamma = 0.2)
rbf.end <- Sys.time()
rbf.time <- rbf.end - rbf.start
rbf.time
}



{# RBF Model Metrics
  library(pROC)
  library(caret)
  # Create predictions on the test set
  svm_rbf_pred <- predict(svm_rbf, newdata = test)
  # Create the confusion matrix
  confusionMatrix(data = svm_rbf_pred, reference = test$legitimate)
  # Extract the accuracy and kappa
  accuracy <- confusionMatrix(data = svm_rbf_pred, reference = test$legitimate)$overall['Accuracy']
  kappa <- confusionMatrix(data = svm_rbf_pred, reference = test$legitimate)$overall['Kappa']
  # Extract the AUC
  svm_rbf_pred_num <- as.numeric(svm_rbf_pred) - 1
  svm_rbf_auc <- roc(test$legitimate, svm_rbf_pred_num)$auc
  cat("Acc:", accuracy, " Kappa:", kappa, " AUC:", svm_rbf_auc)
}

{
  library(caret)
  library(ggplot2)
  
  # Train the SVM model using the train function from the caret package
  svm_rbf <- train(legitimate ~ ., data = train, method = "svmRadial",
                   trControl = trainControl(method = "cv", number = 10))
  
  # Compute variable importance using permutation-based method
  importance <- varImp(svm_rbf)
  ggplot(importance, aes(x = Reorder(row.names(importance), -Overall), y = Overall)) +
    geom_bar(stat = "identity", fill = "forestgreen") +
    labs(x = "Features", y = "Importance", title = "Variable Importance Plot",
    subtitle = "Full SVM ~ RBF")
  
}

{# Sigmoid Model Timed
sigmoid.start <- Sys.time()
svm_sigmoid <- svm(legitimate ~ ., data = train, kernel = "sigmoid", gamma = 0.1, coef0 = 1)
sigmoid.end <- Sys.time()
sigmoid.time <- sigmoid.end - sigmoid.start
sigmoid.time}
{# Sigmoid Model Metrics
  svm_sigmoid_pred <- predict(svm_sigmoid, newdata = test)
  confusionMatrix(data = svm_sigmoid_pred, reference = test$legitimate)
  accuracy <- confusionMatrix(data = svm_sigmoid_pred, reference = test$legitimate)$overall['Accuracy']
  kappa <- confusionMatrix(data = svm_sigmoid_pred, reference = test$legitimate)$overall['Kappa']
  svm_sigmoid_pred_num <- as.numeric(svm_sigmoid_pred) - 1
  svm_sigmoid_auc <- roc(test$legitimate, svm_sigmoid_pred_num)$auc
  cat("Acc:", accuracy, " Kappa:", kappa, " AUC:", svm_sigmoid_auc)
}
#-------------------------------------------------------------------------------
#-------------------------------------------------------------------------------
# Predict the class labels for the test data using each SVM model
linear_pred <- predict(svm_linear, newdata = x_test, decision.values = TRUE)
poly_pred <- predict(svm_poly, newdata = x_test, decision.values = TRUE)
rbf_pred <- predict(svm_rbf, newdata = x_test, decision.values = TRUE)
sigmoid_pred <- predict(svm_sigmoid, newdata = x_test, decision.values = TRUE)
#-------------------------------------------------------------------------------
# Create Confusion Matrices for All kernels
linear_cm <- confusionMatrix(linear_pred, y_test)
poly_cm <- confusionMatrix(poly_pred, y_test)
rbf_cm <- confusionMatrix(rbf_pred, y_test)
sigmoid_cm <- confusionMatrix(sigmoid_pred, y_test)
#-------------------------------------------------------------------------------
library(ROCR)

# Calculate the predicted probabilities for each model
linear_prob <- attributes(linear_pred)$decision.values[,1]
poly_prob <- attributes(poly_pred)$decision.values[,1]
rbf_prob <- attributes(rbf_pred)$decision.values[,1]
sigmoid_prob <- attributes(sigmoid_pred)$decision.values[,1]

# Calculate the FPR and TPR for each model
linear_pred_roc <- prediction(linear_prob, y_test)
poly_pred_roc <- prediction(poly_prob, y_test)
rbf_pred_roc <- prediction(rbf_prob, y_test)
sigmoid_pred_roc <- prediction(sigmoid_prob, y_test)

linear_perf <- performance(linear_pred_roc, "tpr", "fpr")
poly_perf <- performance(poly_pred_roc, "tpr", "fpr")
rbf_perf <- performance(rbf_pred_roc, "tpr", "fpr")
sigmoid_perf <- performance(sigmoid_pred_roc, "tpr", "fpr")

# Create the ROC curve plot
{ par(mfrow=c(2,2))
  # Create the ROC curves for each kernel
  plot(linear_perf, col="red", main="ROC Curve for SVM Linear", lwd=2, xlim=c(0,1), ylim=c(0,1), xlab="False Positive Rate", ylab="True Positive Rate")
  abline(a=0, b=1, lty=2, col="black")
  plot(poly_perf, col="blue", main="ROC Curve for SVM Polynomial", lwd=2, xlim=c(0,1), ylim=c(0,1), xlab="False Positive Rate", ylab="True Positive Rate")
  abline(a=0, b=1, lty=2, col="black")
  plot(rbf_perf, col="green", main="ROC Curve for SVM RBF", lwd=2, xlim=c(0,1), ylim=c(0,1), xlab="False Positive Rate", ylab="True Positive Rate")
  abline(a=0, b=1, lty=2, col="black")
  plot(sigmoid_perf, col="purple", main="ROC Curve for SVM Sigmoid", lwd=2, xlim=c(0,1), ylim=c(0,1), xlab="False Positive Rate", ylab="True Positive Rate")
  abline(a=0, b=1, lty=2, col="black")
}
#===============================================================================
library(ggplot2)

# Combine the data into one data frame
df <- data.frame(
  fpr = c(linear_perf@fpr, poly_perf@fpr, rbf_perf@fpr, sigmoid_perf@fpr),
  tpr = c(linear_perf@tpr, poly_perf@tpr, rbf_perf@tpr, sigmoid_perf@tpr),
  kernel = rep(c("Linear", "Polynomial", "RBF", "Sigmoid"), each = length(linear_perf@fpr))
)

# Create the ggplot
ggplot(df, aes(x = fpr, y = tpr, color = kernel)) +
  geom_line(size = 2) +
  xlim(0, 1) +
  ylim(0, 1) +
  labs(title = "ROC Curves for SVM Kernels", x = "False Positive Rate", y = "True Positive Rate", color = "Kernel") +
  theme_bw() +
  theme(legend.position = "bottom")

#===============================================================================
library(pROC)
library(ggplot2)

# Convert binary factor to numeric vector
y_test_numeric <- as.numeric(as.character(y_test))
linear_pred_numeric <- as.numeric(as.character(linear_pred))
poly_pred_numeric <- as.numeric(as.character(poly_pred))
rbf_pred_numeric <- as.numeric(as.character(rbf_pred))
sigmoid_pred_numeric <- as.numeric(as.character(sigmoid_pred))

as.numeric(as.character(poly_pred))
# Calculate ROC curves and AUC for each model
linear_roc <- roc(y_test_numeric, linear_pred_numeric)
linear_auc <- auc(linear_roc)
poly_roc <- roc(y_test_numeric, poly_pred_numeric)
poly_auc <- auc(poly_roc)
rbf_roc <- roc(y_test_numeric, rbf_pred_numeric)
rbf_auc <- auc(rbf_roc)
sigmoid_roc <- roc(y_test_numeric, sigmoid_pred_numeric)
sigmoid_auc <- auc(sigmoid_roc)

# Create a dataframe with the ROC curve data for each model
roc_data <- data.frame(
  FPR = c(linear_roc$specificities, poly_roc$specificities, rbf_roc$specificities, sigmoid_roc$specificities),
  TPR = c(linear_roc$sensitivities, poly_roc$sensitivities, rbf_roc$sensitivities, sigmoid_roc$sensitivities),
  Kernel = rep(c("Linear", "Polynomial", "RBF", "Sigmoid"), each = length(linear_roc$specificities))
)

# Plot the ROC curves using ggplot2
ggplot(roc_data, aes(x = FPR, y = TPR, color = Kernel)) +
  geom_line(size = 2) +
  geom_abline(intercept = 1, slope = 1, linetype = "dashed", color = "black") +
  labs(title = "ROC Curves for SVM Kernels",
       subtitle = paste0("Linear AUC =", round(linear_auc, 2), 
                         " | Polynomial AUC =", round(poly_auc, 2),
                         " | RBF AUC =", round(rbf_auc, 2),
                         " | Sigmoid AUC =", round(sigmoid_auc, 2)),
       x = "False Positive Rate",
       y = "True Positive Rate",
       color = "Kernel") +
  theme_bw() +
  scale_x_reverse()

#===============================================================================
library(ggplot2)
install.packages("ggpubr")
library(ggpubr)
library(pROC)
# Linear kernel
ggroc(linear_roc, legacy.axes = TRUE, color = "blue") +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "black") +
  labs(title = paste("ROC Curve for SVM Linear Kernel (AUC =", round(linear_auc, 2), ")"),
       x = "False Positive Rate (1 - Specificity)",
       y = "True Positive Rate (Sensitivity)",
       color = "SVM") +
  theme_bw()

# Polynomial kernel
ggroc(poly_roc, legacy.axes = TRUE, color = "blue") +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "black") +
  labs(title = paste("ROC Curve for SVM Polynomial Kernel (AUC =", round(poly_auc, 2), ")"),
       x = "False Positive Rate (1 - Specificity)",
       y = "True Positive Rate (Sensitivity)",
       color = "SVM") +
  theme_bw()

# RBF kernel
ggroc(rbf_roc, legacy.axes = TRUE, color = "blue") +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "black") +
  labs(title = paste("ROC Curve for SVM RBF Kernel (AUC =", round(rbf_auc, 2), ")"),
       x = "False Positive Rate (1 - Specificity)",
       y = "True Positive Rate (Sensitivity)",
       color = "SVM") +
  theme_bw()

# Sigmoid kernel
ggroc(sigmoid_roc, legacy.axes = TRUE, color = "blue") +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "black") +
  labs(title = paste("ROC Curve for SVM Sigmoid Kernel (AUC =", round(sigmoid_auc, 2), ")"),
       x = "False Positive Rate (1 - Specificity)",
       y = "True Positive Rate (Sensitivity)",
       color = "SVM") +
  theme_bw()
#===============================================================================
# Linear SVM Model Tunings
#===============================================================================
# Define the number of folds for cross-validation
k <- 10
# Define vectors to store the accuracy scores and fitted models
costs <- seq(from = 1, to = 10, by = 1)
accuracy <- rep(0, length(costs))
fitted_models <- vector("list", length(costs))
# Loop over the cost values and train the linear SVM model with cross-validation
for (i in seq_along(costs)) {
  # Fit the linear SVM model with the current cost value
  svm_linear_tune <- svm(legitimate ~ ., data = train, kernel = "linear", cost = costs[i])
  # Perform k-fold cross-validation and calculate mean accuracy score
  cv_folds <- createFolds(y = train$legitimate, k = k, list = TRUE, returnTrain = FALSE)
  cv_results <- sapply(cv_folds, function(fold) {
    svm_fold <- svm(legitimate ~ ., data = train[fold,], kernel = "linear", cost = costs[i])
    mean(predict(svm_fold, train[-fold,]) == train$legitimate[-fold])
  })
  accuracy[i] <- mean(cv_results)
  # Save the fitted model to the list of models
  fitted_models[[i]] <- svm_linear_tune
}

# Plot the accuracy scores for different cost values
# Create a data frame with the cost and accuracy values
df <- data.frame(Cost = costs, Accuracy = accuracy)
# Plot the data using ggplot
library(ggplot2)
ggplot(df, aes(x = Cost, y = Accuracy)) +
  geom_line() +
  geom_point() +
  labs(title = "Linear SVM Cost v. Accuracy Comparison", x = "Cost", y = "Accuracy")


# Create a data frame with the cost and accuracy values
df <- data.frame(Cost = costs, Accuracy = accuracy)
# Find the index of the highest accuracy value
max_idx <- which.max(df$Accuracy)
# Plot the data using ggplot
ggplot(df, aes(x = Cost, y = Accuracy)) +
  geom_line() +
  geom_point() +
  geom_point(data = df[max_idx, ], aes(x = Cost, y = Accuracy), color = "forestgreen", size = 5) +
  labs(title = "Linear SVM Cost v. Accuracy Comparison",
       x = "Cost", y = "Accuracy")

# Sort the data frame by Accuracy in descending order
df_sorted <- df[order(df$Accuracy, decreasing = TRUE),]

# Create the bar plot
ggplot(df_sorted, aes(x = Cost, y = Accuracy)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  labs(x = "Cost", y = "Accuracy") +
  coord_flip() +
  labs(title = "Linear SVM Cost v. Accuracy Comparison",
       x = "Cost", y = "Accuracy")

#===============================================================================
library(pROC)
library(ggplot2)

# Train linear SVM models with different cost values
costs <- seq(from = 1, to = 10, by = 1)
models <- lapply(costs, function(c) svm(legitimate ~ ., data = train, kernel = "linear", cost = c))

# Calculate AUC and ROC curve for each model
aucs <- sapply(models, function(m) roc(train$legitimate, predict(m, newdata = train)))
roc_data <- data.frame(fpr = unlist(lapply(aucs, function(a) a$specificities[[1]])),
                       tpr = unlist(lapply(aucs, function(a) a$sensitivities[[1]])),
                       cost = rep(costs, each = length(aucs[[1]]$specificities[[1]])))

# Plot ROC curves for all models
ggplot(roc_data, aes(x = fpr, y = tpr, color = as.factor(cost))) +
  geom_line() +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed") +
  scale_color_discrete(name = "Cost") +
  labs(title = "ROC Curve for Linear SVM Models",
       x = "False Positive Rate",
       y = "True Positive Rate")

#===============================================================================
# Polynomial SVM Model Tunings
#===============================================================================
set.seed(568)
degrees <- seq(from = 2, to = 10, by = 1)
models <- lapply(degrees, function(d) svm(legitimate ~ ., data = train, kernel = "polynomial", degree = d, coef0 = 1))
# Compute accuracies for each model
accuracies <- sapply(models, function(m) mean(predict(m, newdata = test) == test$legitimate))

# Create a data frame with the degrees and accuracies
data <- data.frame(Degree = degrees, Accuracy = accuracies)

# Create the plot
ggplot(data, aes(x = Degree, y = Accuracy)) +
  geom_line() +
  geom_point() +
  labs(x = "Degree", y = "Accuracy") +
  ggtitle("Accuracy of SVM models with polynomial kernel") +
  theme_minimal()

#===============================================================================
# Radial Basis Function (RBF) SVM Model Tunings
#===============================================================================
set.seed(568)
# svm_rbf <- svm(legitimate ~ ., data = train, kernel = "radial", gamma = 0.1)
gamma.tune <- seq(from = 0.1, to = 1, by = 0.1)
models <- lapply(gamma.tune, function(g) svm(legitimate ~ ., data = train, kernel = "radial", gamma = g))
accuracies <- sapply(models, function(m) mean(predict(m, newdata = test) == test$legitimate))

data <- data.frame(Gamma = gamma.tune, Accuracy = accuracies)

# Create the plot
ggplot(data, aes(x = gamma.tune, y = Accuracy)) +
  geom_line() +
  geom_point() +
  labs(x = "Gamma", y = "Accuracy") +
  ggtitle("Accuracy of SVM models with Radial Basis Function kernel") +
  theme_minimal()
#===============================================================================
# Create Visualizations of Model Aspects
#===============================================================================

# Decision Boundary  Plots (tentative)
#===============================================================================
#===============================================================================
#===============================================================================
# Adding Micaela's code
#===============================================================================
cwd <- getwd()
file.name <- "ImportVar10.csv"
paste(cwd, file.name ,sep="/")
df <- read.csv(paste(cwd, file.name ,sep="/"))

#----------------------------------------------------------------------------------------
#Load the data set. It is new a created only using 11 var (10 predictors and 1 outcome)
# I named it ImportVar10
#ImportVar10 <- read.csv("C:/Rprojects/Malware detection project/ImportVar10.csv")
View(ImportVar10)
Malware_df <- ImportVar10 # this is the raw data as it was imported.
str(Malware_df)
#---------------------------------------------------------------------------
#Set the random number seed so we can reproduce the results
#Convert outcome variable to 2-level factor to use it for classification.
ImportVar10$legitimate <- as.factor(ImportVar10$legitimate)
response_var <- "legitimate" 
ImpVar10Classes <- as.factor(ImportVar10$legitimate) 
#Note ImpVarClasses is the outcomme variable which is a factor and I will use it later on when I am training data
str(ImportVar10)
#------------------------------------------------------------------------------
#Splitting the data 
#--------------------------------------------
set.seed(568)
##list = FALSE, a matrix of row numbers is generated.
#These samples are allocated to the training set.I have to indicate a column with the dataset so 
#the createDataPartition works.Creating the predictors df excluding the outcome variable.
predictors <- ImportVar10[,-11]
predictors
trainingindex <- createDataPartition(y=ImportVar10$legitimate, p = .70,list= FALSE) 
head(trainingindex)
traindata <-ImportVar10[trainingindex, ]
testdata <- ImportVar10[-trainingindex, ]
x_train <- traindata[, -which(names(traindata) == response_var)]
y_train <- traindata[, which(names(traindata) == response_var)]
x_test <- testdata[, -which(names(testdata) == response_var)]
y_test <- testdata[, which(names(testdata) == response_var)]
#-------------------------------------------------
#Subset the data into objects for training using.
#trainPredictors1 <- predictors[trainingindex,]
#trainClasses1 <- ImpVar10Classes[trainingindex] 
#Do the same for the test set using negative integers.
#testPredictors1 <- predictors[-trainingindex,]
#testClasses1 <- ImpVar10Classes[-trainingindex]
#str(trainPredictors1) 
#str(testPredictors1)  
#-----------------------------------------------------------------
# Resampling data 
#set.seed(568) 
#repeatedSplits <- createDataPartition(trainClasses1, p = 0.7, times = 30)
#str(repeatedSplits) 
#-------------------------------------------------------------------------------
#To create indicators for 10-fold cross-validation,
#set.seed(568)
#cvSplits <- createFolds(trainClasses1, k = 10, returnTrain = TRUE) 
#str(cvSplits)   
#Get the first set of row numbers from the list. 
#fold1 <- cvSplits[[1]] 
#To get the first 90% of the data (the first fold)
#cvPredictors1 <- trainPredictors1[fold1,] 
#cvClasses1 <- trainClasses1[fold1]
#nrow(trainPredictors1)  [1]  
#nrow(cvPredictors1)  [1]

## Training predictors
#trainPredictors <- as.matrix(trainPredictors1)
#svmPredictors <- svm(x=trainPredictors, y = trainClasses1, k= "linear",
#  preProcess = c("center", "Scale"),
#tuneLength = 10,
#  trControl = train_control)
#svmPredictors # note: Run successfully

#Predict new samples. Running test using svmPredictors
#---------- Running the test predictors - run successfully
#testPredictions <- predict(svmPredictors, newdata = testPredictors1, type = "class")
#head(testPredictions) 
#str(testPredictions)
#---------------------------------------------------------------------
#library(tidyverse)
#

#install.packages("MASS")
#library(MASS)
#install.packages("ipred")
#library(ipred)

# Building models - SVM
#set up repeated k-fold cross Validation

#-----------------------------------------------------------------------
# Create SVM model
# Create the svm model using the data set ImportVar10
set.seed(568)
svmModel <- svm(legitimate~  ., data = traindata, kernel = "linear", cost = 10 )


#svmModel1 <- svm(legitimate~  ., data = traindata) # to test the plot of svm
# plot(svmModel1, traindata) - did not run
#print(svmModel)

# Predicting ---------------------------------------------------
svmPredictors <- predict(svmModel, x_test, decision.values = TRUE)
confusion_matrix <- confusionMatrix(svmPredictors, y_test)
print(confusion_matrix)
print(confusion_matrix$table) # it shows a count of predictions per option of the outcome Var.
svm.accuracy <- confusion_matrix$overall["Accuracy"]
print(svm.accuracy)
svm.kappa <- confusion_matrix$overall["Kappa"]
print(svm.kappa)

# Calculate the ROC curve and AUC
library(pROC)
# Convert binary factor to numeric vector
y_test_numeric <- as.numeric(as.character(y_test))
svmPredictors_numeric <- as.numeric(as.character(svmPredictors))
roc_data <- roc(y_test_numeric, svmPredictors_numeric)
auc <- auc(roc_data)

svm.subtitle <- paste("Accuracy: ", round(svm.accuracy, 4),
                      " Kappa: ", round(svm.kappa, 4))
#---------------------------------------------------------------------------------
# Plot the ROC curve using ggplot2
library(ggplot2)
# if(!require(devtools)) install.packages("devtools")
# devtools::install_github("kassambara/ggpubr")
install.packages("ggpubr")
library(ggpubr)
library(pROC)
ggroc(roc_data, legacy.axes = TRUE, color = "purple") +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "black") +
  labs(title = paste("ROC Curve for SVM (AUC =", round(auc, 2), ")"),
       subtitle = svm.subtitle,
       x = "False Positive Rate (1 - Specificity)",
       y = "True Positive Rate (Sensitivity)",
       color = "SVM") +
  theme_bw()

#^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ code below is old and I can re-use it

train_control <- trainControl(method = "cv", number=10)
set.seed(568)
#train_control

###################################
# creating model. adding kernlab
##library(kernlab)
##library(mlbench)
#Model train using the dataset ImportVar10

#svmModelTrain <- train(legitimate ~  ., data = ImportVar10, 
#      method = "svmRadial",
#      metric = "Accuracy",
#     trControl = train_control)
#svmModelTrain # note: this model run .
# Adding pre-processing to svmModelTrain
#svmModelTrainP <- train(legitimate ~  ., data = ImportVar10, 
# method = "svmRadial",
#  metric = "ROC",
# preProcess = c("center", "scale"))
#svmModelTrainP # note: this model run successfully.
# Tuning parameters
#set.seed(568)
#tuneGrid <- expand.grid(c = c(0.25, .5, 1),
#                      sigma = 0.1) # note: this run successfully.
# Creating traincontrol using 10-fold cross-validation , reaping 5 times
#set.seed(568)
#str(predictors)
#str(ImportVar10)
#svmTune <- train(legitimate ~  ., data = ImportVar10, 
#   method = "svmRadial",
#   preProcess = c("center", "scale"),
#       trControl = trainControl) 
#svmTune
# I kep getting a message saying names are not matching or something like that. 
#?make.names
######################################

