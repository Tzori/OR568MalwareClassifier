#===============================================================================
# Load dependencies/modules/libraries
#===============================================================================
library(tidyverse)
library(dplyr)
library(ggplot)
library(ggplot2)
library(caret)
library(corrplot)
library(lattice)
library(e1071)
#===============================================================================
# Load dataset for your machine 
#===============================================================================
# Set your working directory to the directory to where all project R Scripts
cwd <- getwd()
file.name <- "Kaggle-data-FINAL.csv"
paste(cwd, file.name ,sep="/")
data <- read.csv(paste(cwd, file.name ,sep="/"))
data.load <- data
#===============================================================================
# Pre-Process Data
#===============================================================================
# remove non numeric data
library(dplyr)
data <- data %>% select_if(~!is.character(.))
print(paste("Removed non-numeric column: ", 
            colnames(data.load %>% select_if(~is.character(.)))))

# remove near zero variance
library(caret)
nzv.index <- nearZeroVar(data)
nzv.colnames <- colnames(data[nzv.index])
data <- data[, -which(names(data) %in% nzv.colnames)]
print(paste("Removed near-zero variance column: ", nzv.colnames))

# remove skewed columns
library(moments)
skew <- apply(data, 2, skewness)
threshold <- 1 # Set your desired skewness threshold
skewed_cols <- names(data)[skew > threshold]
data <- select(data, -skewed_cols)
print(paste("Removed skewed columns: ", skewed_cols))

# remove feature specific vars (ID, machine, etc...)
targeted.removal <- c("ID")
data <- data[, -which(names(data) %in% targeted.removal)]

colnames(data)
#===============================================================================
# Split Test Train Data
#===============================================================================
# Sample a subset of the data for training
# Shortened size as the full dataset is too big to partition 211.1Gb
train_idx <- sample(1:nrow(data), size = 10000, replace = FALSE)
train_data <- data[train_idx, ]
# Use the remaining data for testing
test_data <- data[-train_idx, ]

#===============================================================================
# Create Model
#===============================================================================
# Define the SVM model with a linear kernel
svm_model <- svm(legitimate ~ ., data = train_data, kernel = "linear")
# Make binary test predictions
test_predictions <- predict(svm_model, test_data)
test_predictions_binary <- ifelse(test_predictions <= 0, 0, 1)
# Compare the predictions to the actual labels
test_accuracy <- mean(test_predictions_binary == test_data$legitimate)
cat("Accuracy:", test_accuracy)
# Generate the confusion matrix
table(test_predictions_binary, test_data$legitimate)
#===============================================================================
# Validate Model
#===============================================================================

#===============================================================================
# Create Visualizations of Model Aspects
#===============================================================================
# ROC Curve Plot
library(pROC)
roc_obj <- roc(test_data$legitimate, as.numeric(test_predictions_binary))
plot(roc_obj, main = "ROC Curve", col = "blue")

# Decision Boundary 
ggplot(df, aes(x = x, y = y)) +
  geom_point(aes(color = label)) +
  scale_color_manual(values = c("red", "blue", "black")) +
  stat_function(
    aes(group = 1, color = "Decision Boundary"),
    fun = function(x) (-svm_model$coefs[1] - svm_model$coefs[2] * x) / svm_model$coefs[3]
  ) +
  xlab("X") +
  ylab("Y") +
  ggtitle("Decision Boundary")

