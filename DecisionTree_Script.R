# Relevant Libraries
library(AppliedPredictiveModeling)
library(caret)
library(e1071)
library(corrplot)
library(lattice)
library(dplyr)
library(rpart)
library(rpart.plot)
library(pROC)
library(ROCR)
install.packages("Metrics")
install.packages("plotly")
library(dplyr)
library(rpart)
library(rpart.plot)
library(Metrics)
library(mlr)
library(ggplot2)
library(plotly)

# Load dataset
# Set your working directory to the directory to where all project R Scripts 
cwd <- getwd()
file.name <- "Kaggle-data-FINAL.csv"
paste(cwd, file.name ,sep="/")
df <- read.csv(paste(cwd, file.name ,sep="/"))

df = read.csv('/Users/18043/Downloads/Kaggle-data-FINAL.csv')

# Label the response variable column name
response_var <- "legitimate" 
## Confirm there are no nulls
sum(is.na(df))
## Check for near zero variance using the nearZeroVar function
zeroVar <- caret::nearZeroVar(df, saveMetrics=FALSE)
## Column index 10, 30, 31, 32, 33, and 47 all have zero  variance
colnames(df[c(10,30,31,32,33,47)])
##Dropping near zero variance columns
df_trans1 <- df[, -c(10,30,31,32,33,47)] 
##Check to see what class each attribute is
sapply(df_trans1, class)
##Can see that md5 is a character class so we will drop this as it is not needed
df_trans2 <- df_trans1[,-2]
colnames(df_trans2)

## Find highly correlated variables; remove corr magnitude .8 or higher
corr_matrix <- cor(df_trans2)
high_corr <- caret::findCorrelation(corr_matrix, cutoff = 0.8)
removed_variables <- names(df_trans2)[high_corr]
removed_variables

df_trans3 <- df_trans2[, -high_corr]
dim(df_trans3)
head(df_trans3)


## Now that all variables have been standardized, a test train split will be created
## I will be using a 70/30 split
set.seed(568)
# Create the partition index with a 70/30 split
index <- caret::createDataPartition(y = df_stdz$legitimate, p = 0.7, list = FALSE)

# Create the train and test sets
train <- df_stdz[index, ]
test <- df_stdz[-index, ]
## Create the df with X and Y variables
# Remove "legitimate" column from training and test datasets
x_train <- train[, -which(names(train) == response_var)]
y_train <- train[, which(names(train) == response_var)]
y_train <- factor(y_train)
x_test <- test[, -which(names(test) == response_var)]
y_test <- test[, which(names(test) == response_var)]



####################
####################
####################
#Model tuning
####################


ParamHelpers::getParamSet("classif.rpart")

d.tree.params <- mlr::makeClassifTask(
  data=train, 
  target="legitimate"
)

param_grid <- ParamHelpers::makeParamSet( 
  ParamHelpers::makeDiscreteParam("maxdepth", values=1:30))

# Define Grid
control_grid = mlr::makeTuneControlGrid()
# Define Cross Validation
resample = mlr::makeResampleDesc("CV", iters = 3L)
# Define Measure
measure = acc

#https://towardsdatascience.com/decision-tree-hyperparameter-tuning-in-r-using-mlr-3248bfd2d88c
set.seed(568)
dt_tuneparam <- mlr::tuneParams(learner='classif.rpart', 
                           task=d.tree.params, 
                           resampling = resample,
                           measures = measure,
                           par.set=param_grid, 
                           control=control_grid, 
                           show.info = TRUE)

result_hyperparam <- generateHyperParsEffectData(dt_tuneparam, partial.dep = TRUE)

library(ggplot2)
library(plotly)
ggplot(
  data = result_hyperparam$data,
  aes(x = maxdepth, y=acc.test.mean)
) + geom_line(color = 'darkblue')


param_grid_multi <- makeParamSet( 
  makeDiscreteParam("maxdepth", values=1:30),
  makeNumericParam("cp", lower = 0.001, upper = 0.01),
  makeDiscreteParam("minsplit", values=1:30)
)
#runs for hours
dt_tuneparam_multi <- tuneParams(learner='classif.rpart', 
                                 task=d.tree.params, 
                                 resampling = resample,
                                 measures = measure,
                                 par.set=param_grid_multi, 
                                 control=control_grid, 
                                 show.info = TRUE)
#[Tune] Result: maxdepth=17; cp=0.001; minsplit=19 : acc.test.mean=0.9689460

dt_tuneparam_multi


#################################
#################################
#################################
#Final tree after tuning
#adding minbucket and CV
library(rpart)
library(rpart)
library(rpart.plot)
library(Metrics)
library(mlr)
library(ggplot2)
library(plotly)
startTime <- Sys.time()
final_tree <- rpart( y_train ~ .- ID, data=x_train, method="class", control=rpart.control(cp=0.001,maxdepth=17, minsplit = 19, minbucket = 10, functions = lrFuncs,
                                                                                          method = "repeatedcv", ## Repeated CV
                                                                                          repeats = 2,
                                                                                          number = 3, ## Number of folds
                                                                                          verbose = FALSE) )
endTime <- Sys.time()
endTime - startTime
#tree plotting
prp(final_tree, space=4, split.cex = 1.5, nn.border.col=0)

library("partykit")
final_tree_plot = as.party(final_tree)
plot(final_tree_plot)

# predict legitmate with this classification tree: 
final_tree_train = predict(final_tree,newdata=x_train, type='class')


## performance evaluation
final_tree_train_results = caret::postResample(pred=final_tree_train, obs=y_train)
final_tree_train_results

# predict legitmate with this classification tree: 
final_tree_test = predict(final_tree,newdata=x_test, type='class')


#confusion matrix
#TRAIN DATA
final_tree_train = predict(final_tree,newdata=x_train, type='class')
caret::confusionMatrix(final_tree_train, y_train)
#TEST DATA
final_tree_test = predict(final_tree,newdata=x_test, type='class')
caret::confusionMatrix(final_tree_test, as.factor(y_test))

#ROC
#value of 2 here predicts legit=1, change it to a 1 and it predicts legit=0
tree.preds.final <- predict(final_tree, x_test, type="prob")[, 2]
tree.roc.final <- pROC::roc(y_test, tree.preds.final)
print(tree.roc.final)
plot(tree.roc.final, main = "Decision Tree ROC \n AUC = 0.9816")


## performance evaluation
final_tree_test_results = caret::postResample(pred=final_tree_test, obs=as.factor(y_test))
final_tree_test_results

rpart.plot(final_tree)
printcp(final_tree)
plotcp(final_tree)

varImpTree <- caret::varImp(final_tree)
varImpTree
# Create a data frame with variable names and importance scores
var_df_tree <- data.frame(variable = row.names(varImpTree),
                      importance = varImpTree$Overall)

# Convert the variable column to a factor with ordered levels in reverse order
var_df_tree$variable <- factor(var_df_tree$variable, levels = rev(var_df_tree$variable))

# Sort the data frame by importance scores in descending order
var_df_tree <- var_df_tree[order(var_df_tree$importance, decreasing = TRUE),]

ggplot(var_df_tree, aes(x = reorder(variable, +importance), y= importance)) + 
  geom_bar(stat = "identity", color="black", fill = "forest green") +
  labs(title = "Variable Importance Plot",
       subtitle = "Classification Tree",
       xlab("Predictor"), ylab("Importance Score")) +
  xlab("Predictor") + ylab("Importance Score") +
  coord_flip() +
  theme(axis.text.x = element_text(angle = 0, vjust = 0.5, hjust = 1))

